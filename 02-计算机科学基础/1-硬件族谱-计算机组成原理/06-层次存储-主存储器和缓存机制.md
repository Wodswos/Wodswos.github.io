> 理想的存储器：更大、更快、更便宜的非易失性存储器。
>
> 实际的存储器：鱼和熊掌难以兼得，通过金字塔式的存储结构获得最好的性价比。

也许有一天会出现那么一种理想的存储器：访问速度足够快（甚至仅局限于光速这种物理限制），大小也能比现有的硬盘更易扩充，制造成本也不高，断电数据还不会丢失——那么今天已有的存储结构就该成为历史的尘埃了。

但既然不存在这样的理想存储器，那就面对现实，脚踏实地地用现有条件去设计一个最优解：金字塔式的多级存储结构。

![image-20210324124311673](C:\Users\Five\Desktop\note\img\image-20210324124311673.png)

也是源于这样的金字塔存储结构，在开发中往往要求程序员尽量满足局部性原则，从而使得程序可以得到更好的性能。

* 时间局部性：最近被访问的存储单元很快会被再次访问
* 空间局部性：正在被访问的存储单元附近的存储单元很可能会在短时间内被访问

> 关于局部性原则，个人在操作系统-虚拟内存部分有更多笔记。

> 倒也不一定是局部性原理——只要访问的地址不是完全随机（每个地址在每一刻有相同的概率被访问），层次存储就会比较好地起到作用。越不随机，效果越好。

Amdahl's Law，一个很简单的推导，描述部分的提升对于整体的影响.

一个部分x在整体访问中所占的时间比例为$\alpha$，则该部分效率提升$k$倍后整体的提升为：
$$
S = \frac {T_{old}} {T_{new}} = \frac {T_{old}} {T_{old}[(1-\alpha) + \alpha /k]} = \frac 1 {(1-\alpha) + \frac \alpha k}
$$







先写RAM，Random Access Memory，其名字也说明了这种存储器的一个特性：任何一个地址的访问（随机访问）都是一样快的。

RAM主要有两种，SRAM（Static RAM）和DRAM（Dynamic RAM），其物理组成不同，速度也天差地别。

# SRAM

90年代，DRAM的速度逐渐开始跟不上CPU，而SRAM却在速度上有着良好的发展和表现。

于是，SRAM常用作高速缓存Cache，是存储结构的重要组成部分。

![image-20210324124027652](C:\Users\Five\Desktop\note\img\image-20210324124027652.png)

SRAM的访问速度约为3个时钟周期

## SRAM存储单元

SRAM靠双稳态触发器构成存储单元——所以访问是很块的。





## CAM

Content Addressable Memory，内容地址存储器。

在每个存储单元都包含了一个内嵌的比较逻辑，写入CAM的数据会和器内部存储的每一个数据进行比较，并返回与端口数据相同的所有内部数据的地址。[^4]

CAM的应用也比较广泛，比如在路由器中的地址交换表，还有……TLB？

# DRAM

Dynamic Random Access Memory

![image-20210324123120534](C:\Users\Five\Desktop\note\img\image-20210324123120534.png)

DRAM靠电容存储，速度相比于晶体管的SRAM自然慢很多。

## 存储单元







电容存在漏电效应，经过一段时间（几微秒）后电容上的电荷会流失，导致所存信息丢失。

所以需要定期对所有单元进行刷新，使原来表示逻辑“1”电容上的电荷得到补充，而原来表示逻辑“0”的电容仍保持无电荷状态。

> 需要动态刷新，所以称为Dynamic。



集成度高，功耗较低，价格较低。

速度较慢，定时刷新。



## 地址译码





## SDRAM

Synchronous Dynamic Random-Access Memory，同步动态随机存取内存。

> 注意区分此处S代表的是Synchronous而不是Static



一次访问需要经过如下过程：

* Bus Request
  * CPU申请总线，然后将地址发给内存控制器
  * 内存控制器将CPU的地址分解成一个行地址和一个列地址等多个部分
* [Precharge]
  * 行预充电（关闭行）
  * 这个过程中耗费的时间称为tRP，RAS（Row Access Strobe） Precharge
    * 在PC133标准中，2~3个时钟周期，约15-23ns
* Row Access
  * 内存控制器发出行地址被Row Decoder接收
  * 选中存储阵列中对应的行，这一行的所有信号会被放大，放入缓冲区
    * 称为ACT（Active，激活）或者RAS（Row Access Strobe，行访问）
  * 这个过程中耗费的时间称为tRCD，Row to Column Delay，从行选到列选的延迟
    * 在PC133标准中，2~3个时钟周期，约15-23ns
* Column Access
  * 发送列地址到Column Decoder
  * 从缓冲区中选出对应的那一列，如果此时是读操作，则该数据会被送到数据输出接口上去
  * 这过程耗费的时间称为CL，CAS（Column to Access Strobe） Latency
    * 在PC133标准中，2~3个时钟周期，约15-23ns
    * 和tRCD，tRP不同，CL单位一般默认是时钟周期，而不是ns。
  * CL个时钟周期后，DRAM送出第一个数据，随后每个时钟周期DRAM送出一个数据。
    * 数据宽度取决于内存条上有几个DRAM芯片
* Bus Transmission





PC133表示时钟频率133MHz，周期7.5ns。PC150、PC166同理





## DDR

Double Data Rate，双倍数据速率

DDR SDRAM即双倍速率同步动态随机存储器。

在时钟的上升沿和下降沿都传输数据。





DDR2-400：核心频率只有100MHz，I/O频率200MHz

DDR2到DDR3也是类似的过程。



### DDR如何起到作用

依旧是前面提到的局部性原理。



## 交叉编址存储器





# 高速缓存Cache

虽然一般来说（包括接下来的内容）Cache通常指SRAM，但我更倾向于将两者分开写，为了强调我自己个人的这样一个想法：

所有所谓的Cache——SRAM和DRAM的关系和交互，都可以、也确实被同理映射到DRAM和磁盘的协同合作——也就是被称为虚拟存储器（Virtual Memory）的东西，哪怕两者在实现上有这样那样的区别。

即Cache更应该被视为一种抽象的方法，而不是某一具体实物的属性。Cache像是层次存储方案背后的一种——普适性？通解？我也不知道如何描述。

## 为什么Cache能起作用

答案已经是老生常谈了，就是程序的时间局部性和空间局部性。

> 无论是高速缓存还是虚拟存储器，都是利用地址访问不完全随机的特性，使得接下来会更可能被访问的地址放到离CPU更近的位置，使其访问代价更低，从而得到更好的优化。
>
> 如何知道接下来哪些地址更可能会被访问？对于缓存而言最主要的依据自然就是程序的时间局部性和空间局部性。

> 但访问的不随机性当然也不仅仅是程序的时间局部性和空间局部性带来的——比如TLB？不管当前运行程序A还是程序B，TLB始终在那里，虽然里面的内容可能会被换，因为它一直很高频。

> 是不是还可以从信息论的角度来理解，分层存储就是一种变长编码，最终总执行（访问）时间就是编码后的文件大小。
>
> 你出现概率大，编码短一点，对应到缓存就是离CPU近一点，访问时间短一点。

注意到SRAM以数据块（或者称为行）为单位和内存DRAM进行数据同步和交换，在虚拟存储器中单位就是页。

为什么要以数据块为单位？

将此次访问的、原来不在缓存中的存储单元放入缓存，利用的是程序的时间局部性：被访问的数据很有可能会在短时间内再次被访问。

而不仅仅将该存储单元放入缓存，还将附近的整个数据块一并放入缓存（以数据块为单位），利用的是程序的空间局部性：被访问的数据周围的数据很有可能会在短时间内再次被访问。

两个局部性原理加一起就是：被访问的数据及其周围的数据很有可能会在短时间内再次被访问。

> 对于虚拟存储器，固定数据块的大小，好像还能有减少内存碎片的效果？算无心插柳柳成荫吗？

举个栗子，如数据块大小为16字节，则访问类似从0x2130H到0x213FH这16个地址时，都会将0x2130H到0x213FH这十六个字节作为一个数据块一并换入缓存（16字节对齐）

同理，如果数据块大小为64字节，则访问0x2100H到0x213FH这64个地址中任意一个时都会将这64个字节一并放入缓存

## 如何实现一个Cache？

知道了为什么Cache能起作用，自然是该实现一个缓存了。

> 那么问题来了，当CPU需要访问一个地址的时候，怎么知道它在不在缓存中呢？

需要注意到这样一个基本事实：所有高层存储的数据来自于低层存储——即高层存储中的数据一定也存在于底层存储，而且最终要归于低层次存储。

所以CPU要确切地访问某一存储单元的时候，显然应该拿着低层次存储器的地址去访问——因为高层次存储的存储数据是不全的，你不一定找得到你要的数据。

**所以缓存中不仅要存储数据块，还应该存储标识该数据块在低层存储器的地址信息。**然后当你拿着低层次存储器的地址的时候，可以去跟高层次存储器的数据块的标识信息比对，对上了就直接把数据取走。

以SRAM为例，其内部像表格一样组织起来，每一行包括有效位、标签、数据（块）等信息，就像这样子

![](C:/Users/Five/Desktop/note/img/image-20210327115312238_crop.png)

然后通过标签+表项，唯一标识一个数据块在低层次存储器的物理地址。

还是以0x2130H-0x213FH这块数据为例，末位信息不用记录（末位对应的是数据块内的位置），我们需要记录的信息是0x213H这个十六进制数。

假设有16个表项，我们可以用表项标识倒数第二个位：如对于0x213H，则将该块地址放入第3条表项。

用标签标记剩下的地址信息：如记录21H。

> 当然也可以直接用标签记录地址信息，但总归是要精益求精的。

最终CPU拿着地址0x2132H来找的时候，直接看第三个表项的是否有效、标签是否为21H，如果是，则表示找到对应数据，如果不是，则去主存找，并将整个0x2130H到0x213FH的数据块填充或替换到表项3。

到此算是解决了CPU怎么找的问题，还解决了新来的放哪里的问题：根据地址找到对应的表项（该放的位子）。

如此一来，缓存已经能够正常工作了：

![image-20210327115312238](C:\Users\Five\Desktop\note\img\image-20210327115312238.png)

## 优化

更系统的优化内容打算另起一文写。这里简单写一些内容。

### 写策略

很容易可以穷举出写数据可能面临的情况：

* Cache命中
  * 写穿透：同时更新Cache和内存对应地址内容
  * 写返回：数据块被替换时才一并将数据写回主存
    * 目前似乎基本都用Writeback
* Cache失效
  * 写不分配：不将要写的部分读入Cache，直接写入主存
  * 写分配：先读入Cache，再写Cache
    * 在满足局部性原理的情况下，这种策略会有更好地性能

### 多路组相联

一般来说，缓存都是根据数据块物理地址映射到对应的表项中，但可能会遇到一种比较尴尬的情况：有两个数据块映射到同一个表项（可以理解为哈希值相同），而CPU又需要不断地交替访问这两个数据块，此时缓存相当于起不到作用——每次都要访问内存交换数据块。

针对这种情况可以有多路组相联，如二路组相联、四路组相联

即每一个行有多个表项（或者说每一个表项有多个数据），当然每一个数据块对应的标签值不一样，不影响数据块的唯一标识

如此一来，CPU可以同时保存两个、四个、N个映射地址相同的数据块，很大程度上降低了这种低性能场景发生的可能性。



#### 替换算法

对于多路组相联，需要有替换算法。

Cache替换算法：随机、轮转、LRU（Least Recently Used）

其中显然LRU性能较好，但需要硬件记录访问历史信息，硬件设计会相应复杂。





## 性能的平衡

![image-20210324142958540](C:\Users\Five\Desktop\note\img\image-20210324142958540.png)

最终访存性能可通过平均访存时间来评价：

Average Memory Access Time = Hit Time  + Miss Penalty * Miss Rate

> 因为有没有命中是访问了缓存后才知道的，而不是你事先就知道在不在缓存里，所以无论如何Hit Time是无法避免的。
>
> 所以此处不是Average Memory Access Time = Hit Time * Hit Rate + Miss Penalty * Miss Rate

而Hit Time和Miss Rate不可兼得：

想要降低Hit Time会限制Cache容量的提升，而Cache容量的降低必然会导致Hit Rate大幅下滑，即Miss Rate升高。



### 命中率的重要性

![image-20210324144305257](C:\Users\Five\Desktop\note\img\image-20210324144305257.png)

提高2%是相对于全体指令的，而对于内存而言，从97到99的命中率，其实减少了整整66%的工作量。

增加命中率，即减少访问内存的频率，相当于提高了内存的访问速度。

再然后通过之前提到的Amdahl's Law，就能能解释性能的提升——访问内存占的时间比重是很大的，所以对整体性能的提升会很明显。



# 缓存的基础优化

因为
$$
存储器平均访问时间=命中时间+缺失率\times 缺失代价
$$


所以明显可以有三个优化方向

* 减少命中时间：较大的块；较大的缓存；较高的关联度
* 降低缺失率：多级缓存；为读取操作设定高于写入操作的优先级
* 降低缺失代价：索引缓存时避免地址转换

当然，在分方向优化各个指标的时候，不能导致另一个方向的指标有明显的退化——但事实上就是没有免费的午餐，每一种优化都是针对大部分情况（或者说总体）而言的，在特定情况下都可能会反而使得性能下降——就像任何一个机器学习的最优化问题一样。

## 减少命中时间





## 降低缺失率

Cache失效的三种情况

* 义务失效：第一次访问时的失效，也即冷启失效
* 容量失效：Cache容量不够保存新的数据块
* 冲突失效：多个存储器位置映射到同一Cache位置







* L1缓存进一步拆分指令和数据Cache

![image-20210324145814462](C:\Users\Five\Desktop\note\img\image-20210324145814462.png)s



## 降低缺失代价





## 现代的CPU

Skylake microarchitecture

> Intel Skylake是英特尔第六代微处理器架构，采用14纳米制程，是Intel Haswell微架构及其制程改进版Intel Broadwell微架构的继任者。

下图为Intel Skylake微处理器架构的Cache参数（算是目前的主流处理器参数了），来自Intel文档《64-IA-32-Architecture-Optimization-Manual》

![image-20210326173859713](C:\Users\Five\Desktop\note\img\image-20210326173859713.png)



### 多核CPU和Smart Cache

在目前主流的多核架构中，L1和L2 Cache是由每个CPU独立管理的，而L3 Cache是多核共享的。



Intel所谓的Smart Cache，







# 高级优化



[^1]: 《计算机组成与设计：硬件/软件接口》David A. Patterson, John L. Hennessy
[^2]: 《计算机体系结构：量化研究方法》作者同上一本书
[^3]:
[^4]:http://blog.sina.com.cn/s/blog_629008650100mg7m.html
[^5]:杂谈闪存二：NOR和NAND Flash - 老狼的文章 - 知乎 https://zhuanlan.zhihu.com/p/26745577
[^6]:杂说闪存一：关公战秦琼之 UFS VS NVMe - 老狼的文章 - 知乎 https://zhuanlan.zhihu.com/p/26652622
[^7]:逻辑扇区和物理扇区的对应关系有点不懂？ - 木头龙的回答 - 知乎 https://www.zhihu.com/question/67935624/answer/258057176