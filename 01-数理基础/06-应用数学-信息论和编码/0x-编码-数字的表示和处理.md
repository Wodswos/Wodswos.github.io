计算机中主要有数字、字符、指令等类型的数据，他们最终都用01的比特流表示。

同样一串01比特流，可以表示一个数字，可以表示一个指令，也可以表示一个字符。计算机怎么去理解、怎么去处理这串01比特流，是需要你告诉它的。

你告诉它这几个字节表示的是一条指令，他就取指，译码，执行；你告诉它这几个字节表示的是一个数字，它就能对其进行算术运算；你告诉他这几个字节代表一个字符，它也能完成相应的操作。



对于自然数的表示很简单——一个进制转换就是了，从十进制到二进制。

# 补码和负数

但负数该如何表示，或者说如何编码呢？

当然我们可以说32位的INT前一半分配给正数用于编码0到$2^{31}-1$，后一半给负数。也可以理解为给32个bit的第一个比特用于编码正负号。

但这样的INT如何运算，或者说运算效率高吗？

当两个比较小的正数用电路里的全加器也许还能勉强得到正确的结果，那一正一负呢？两个负数呢？凭直觉也不像是能得到正确的结果。

> 这种简单粗暴的编码方式还有更多不足的细节[^2].

## 权值角度

我们可以从权的角度去重新考虑问题、制定方案，如表所示赋予权值[^2]：

| $-2^{n-1}$ | $2^{n-2}$ | $2^{n-3}$ | $\dots$ | $2^2$ | $2^1$ | $2^0$ |
| ---------- | --------- | --------- | ------- | ----- | ----- | ----- |
| 1          | 1         | 1         | $\dots$ | 0     | 1     | 0     |

其按位计算权值并相加即可得到表示数，如上表中的数为$-2^{n-1} + 2^{n-2}+2{n-3} + \dots +2^1$，显然这是一个负数。

当我们这样去编码存储计算机中的二进制数时，很容易发现正负数之间的加减法——只要不溢出，都能用电路中的全加器正常计算。

对于这种编码方式，正数只需转换到二进制，然后高位取0即可，也不需要啥额外的操作。

但负数会稍微麻烦——当然我们可以去凑权值，但我们自然希望能有一套比较方便无脑暴力的规则，所幸，的确有这样的一套规则：将负数的绝对值像正数一样操作，再按位取反，末位加1。

如-1的绝对值（原码）为$00 \dots 001$，按位取反+1后得到补码$11\dots111$，

这也就是熟悉的补码。

> 此时0的表示为$000\dots00$，而$10000\dots 000$表示的可不是-0，而是所能表示的最大的那个负数$ - 2^{n-1}$

> 当一正一负两个数相加，正数的绝对值比较大时，会进位，这种进位真的合理吗？抛开负数的最高位权值$ - 2^{n-1}$不看，后面的也是个正数吧？两个正数相加后向一个权值为负的位去进位了？
>
> 但这种进位的结果是喜大普奔的——最高位进到2，然后要再进一位——这时溢出了。这种溢出，是在设计之中的，或者说就是靠着这种溢出完成了负数和正数的运算。溢出之后，重新归正了。
>
> 如果不溢出，我们能给出更高位的权值吗——显然给正的权值不对，给负的也不对，好像也不是不能给，也许可以给个0的权值——即权值$ = - 2^{n-1}+2^{n-2} + 2^{n-2} = 0$。显然，权值位0，那溢出不要就不要了。

> 还有一种溢出，两个正数或者负数相加——也就是真正的溢出，这是我们需要避免的。以INT为例，一般而言是32位（除了早期的16位机子上INT为16位）。
>
> 01111001010100101010010010111001 + 01111001010100101010010010111001，这两个正数相加最高位会得到进位1，变成一个负数。
>
> 而10111001010100101010010010111001 + 10111001010100101010010010111001 ，这样的两个负数相加，两个最高位相加进位溢出后，会得到一个正数——显然也不正确了。

## 循环角度

所谓循环，就是n位的负数的补码看作该负数加上$2^n$，就像多走了一圈，如8位的有符号数中，-2（-00000010）的补码就是无符号的$-2 + 2^8=254 = (11111110)_2$。

> 就像一个钟表一样，从3走到2需要多久？-1个小时？但钟表不能倒着走呀，所以就是（12-1=11）个小时，这里的12就类似$2^n$这样一个周期。

这种角度在理解为什么是“按位取反末位加一”得到负数补码 和 计算机底层补码的乘法运算（低位）的正确性 这两点上有奇效。

> 还是以-2为例，$-2+2^8 = 2^8-1 -2 + 1 = (11111111)_2 - (00000010)_2 + 1$，而$(11111111)_2-(00000010)_2$就相当于一次**按位取反**，从$2^8$到$(11111111)_2$减了一，所以要补上，即**末位加一**。
>
> 当然$(11111111)_2-(00000010)_2$也可以理解为$(00000010)_2$和$(11111111)_2$的一次异或，和1的异或，就是取反嘛。

> 对于负数x，其补码为$2^n + x$，如此一来，$x_{complement}\times y = 2^n y + xy$，因为最后结果需要模$2^n$，所以$x \times y \equiv x y \mod 2^n$
>
> 若y也为负数，则$x_{complement} \times y_{complement} = 2^n\times y + 2^{2n} + 2^n \times x + x\times y \equiv xy \mod 2^n$



## 大小端

大端模式（Big Endian），是指数据的高字节保存在内存的低地址中，而数据的低字节保存在内存的高地址中，这样的存储模式有点儿类似于把数据当作字符串顺序处理：地址由小向大增加，而数据从高位往低位放；这和我们的阅读习惯一致。

小端模式（Little Endian），是指数据的高字节保存在内存的高地址中，而数据的低字节保存在内存的低地址中，这种存储模式将地址的高低和数据位权有效地结合起来，高地址部分权值高，低地址部分权值低。

* 大端：符合阅读习惯
* 小端：将地址高低和数据位权有效结合

常用的x86结构是小端模式，而KEIL C51则为大端模式。很多的ARM，DSP都为小端模式。有些ARM处理器还可以由硬件来选择是大端模式还是小端模式。

| 大小端        | CPU               |
| ------------- | ----------------- |
| Big Endian    | PowerPC、IBM、Sun |
| Little Endian | x86、DEC          |

网络通讯协议都是使用Big-Endian的编码。[^3]



# 浮点数

> 小时候一直有疑惑为什么有限小数和无限循环小数，观感上差了不止一点点——就好像跨越了一个数域，但书上、老师从不强调有限小数和无限循环小数有啥本质差别。
>
> 后来学了数制——在十进制下的有限小数到了其他进制下极有可能是无限循环小数，其他进制下的有限小数到了十进制下也很有可能是无限循环小数。
>
> 这才算对**有理数**这个域有了一些整体的认识，对为什么是从**整数**跨到**有理数**，中间没有另一个数域停留有了一些认识。

> 当然直接从历史上看数域扩张会更直观。有理数的就可以视为分数（整数看作分母为1的特例）——他可不在乎你分出来之后写成小数是怎样的，就算无限循环，也不影响其理解。
>
> $\frac 1 3$虽然在十进制中是个无限小数，但在三进制中却可以是个有限小数$0.1$，但无论如何，我们依旧能为这个分数找到其现实的例子——将一个蛋糕分成三等分。这使得我们能直观理解分数的意义。
>
> 直到无理数的出现，才有了著名的第一次数学危机，也使得数域进一步扩张。

> 进制和权
>
> 尤其是对于小数点后的内容，用权的角度去理解会格外方便。
>
> 如二进制的$0.1$转换为十进制就是$2^{-1}\times 1 =0.5$，三进制的0.1就是$3^{-1}= \frac 13  =0.3333\dots 3333$。

浮点数并不等于有理数——只能说在计算机中用浮点数来尽可能地表示有理数。毕竟再小一段区间的有理数数量也是无穷的，显然不可能用有限的bit位来编码表示无限的有理数。

IEEE Standard 754, a carefully crafted standard for representing floating-point numbers and the operations performed on them.[^1]

这项工作开始于1976年，为Intel的8087（8086的float版）。下式展示了浮点数的表示方法：
$$
V = (-1)^S \times M \times 2^E
$$
![image-20210322205011520](C:\Users\Five\Desktop\note\img\image-20210322205011520.png)

对照上式和上图（Standard floating-point formats），式子是编码基础，图代表具体实现：

* S为符号位，为零时为正，为1时为负。
  * 对应图中s的一个bit位，没啥trick需要注意的
* E是指数
  * 其意义可以理解为左右移动调整M中的小数点的范围——也是称之为浮点的原因
  * 图中exp部分表示一个二进制无符号数e
    * E和e的转换关系为：$E= e - Bias，Bias=2^{k-1}-1$，k为exp部分的长度
      * 为什么用这样的“移码”表示阶呢，一种说法是为了方便比较两个浮点数的大小：符号位相同的情况下，只需从第二高位开始依次比较每一位的大小（比完exp部分比frac），出现不一样的即可分出大小。
    * 在单精度中用8位，即e的范围为0到$2^8-1$，Bias值为$2^7-1$
      * 故E的范围（也即float小数点左右移动范围）为$-2^7 + 1$到$2^7$，即-127到128
      * 但实际上实际上是-126到127，因为当exp全为0或全为1时浮点数另有含义
    * 这么来看，float能表示的最小非零正数为$1.00\dots 001 \times 2^{-126}$（实际上可以更小），最大正数为$1.1\dots11 \times 2^{127}$
    * 双精度中用11位，e的范围为0到$2^{11}-1$，Bias值为$2^{10}-1$
* M为一个介于1和2之间的数，即初始小数点左边永远是1。
  * 比如数1.1010101，而0.10101（二进制）这种应该表示成1.0101，（即小数点右移）再对E减一
    * **This representation is a trick for getting an additional bit of precision for free.**[^1]
  * 在图中对应frac部分
    * 实际上，frac只记录M小数点后的部分，小数点前的部分默认为1
      * 这就是M应该要介于1和2之间的原因，白嫖一个二进制有效数字。
    * 在单精度中用23位，双精度中用52位
      * 即有效数字分别可以达到为24位和53位（二进制），折合十进制有效数字6/7位和15/16位。



前面提到的Normalized Values，即exp部分不全为0或全为1，实际上Float还有两种特殊情况。

* Denormalized Values
  * 当exp部分全为0时，表示Denormalized Values
  * 此时可以认为e=0，E=1-Bias=$2-2^{k-1}$，M = frac
    * 虽然这种情况下e没啥用了，E也不是e-Bias了，但CS:APP上好像是认为此时e=0
    * 此时M不再默认在1到2之间了，可以是0到2之间的任意数
    * frac若为零，则表示M=0，也没有那个默认的1了，**使得float表示0成为可能**
      * 事实上此时整个float也确实所有位为0，用来表示0也很合适
      * 当然由于符号位，所以同时存在+0和-0
    * 显然，此时**可以表示一个非常接近0的数**
      * 此时frac的编码$00\dots001$就意味着$0.0\dots001$而不是$1.0\dots001$
* Special Values，特殊值
  * 当**exp部分全为1**时，表示Special Values
  * 此时倒也没啥M和frac的映射关系了，**frac部分为零时表示无穷**（Infinity），**frac不为零时表示NaN**（Not a Number）
    * 当运算溢出时，可以表示为Infinity
    * 当计算结果不是实数（如$\sqrt {-1}$）时，可以return一个NaN



以下是IEEE Std 754-2008的截图

![image-20210323150729898](C:\Users\Five\Desktop\note\img\image-20210323150729898.png)



## 浮点数的分布

有得必有失，得到了更大的表示范围，自然会在精度上有所损失（尤其是大数），下图展示了浮点数编码的分布特点[^1]：

* FLOAT的frac部分一共就23个比特，24个二进制精度，意味着大于$2^{24}$的INT就（大部分）无法在FLOAT中找到一一对应了
  * 对于n个比特的frac，最小的不能精确表示的Integer是$2^{n+1}+1$
  * $2^{n+1}$这个数勉强算能表示，虽然实际上已经丢失了一个精度了，但丢失了是个二进制的0，然后进到了E上，姑且算没丢
* INT到Double倒是能一一对应，毕竟Double的frac部分比INT还长

![image-20210323155014551](C:\Users\Five\Desktop\note\img\image-20210323155014551.png)

下图展示了编码（Bit representation）和实际的数之间的对应关系[^1]：

![image-20210323155417734](C:\Users\Five\Desktop\note\img\image-20210323155417734.png)



## Rounding

浮点数终究是离散的编码，妄图完美表示连续的实数是不可能的。

也许可以说浮点数和INT没什么不同，当我们试图用INT或浮点数去近似实数时，都需要舍去一定的精度，当然INT的表示范围要小得多，而且对小数的支持也不好，从各方面来说，FLOAT都是一种比INT好得多的实数近似法（或者说更具有普适性）。

既然是近似，就会需要Rounding（中文应该是舍入？）。用INT去表示一个小数，其Rounding很好理解，一般来说就是小学就教的四舍五入，当然也可以选择一律抹掉小数等方法。

The IEEE floating-point format defines four different rounding mode.[^1]

* Round-to-even
  * 向偶数舍入，也可以理解为最近舍入
  * 但当在正中间时，比如2.5和3.5舍入到INT，他会向靠近偶数（even）的一方舍入，即这两个例子分别舍入成2和4
* Round-toward-zero
  * 抹掉后面剩下的
  * 正数的表现和Round-down一样，负数的表现和Round-up一样
* Round-down和Round-up
  * 向下舍入、向上舍入，顾名思义，不赘述了



为什么Round-to-even，而不是四舍五入？

CS:APP给的说法是，如果一堆统计数据中有大量x.5，都向上进，可能会造成统计偏差，Round-to-even有舍有入，一定程度上可以减轻这种偏差。



> 关于具体的计算，就放到组成原理部分的笔记了。

# 数据类型长度

回想大一刚入学的时候对这些数据类型的长度还是迷迷糊糊的，也不愿意去硬记——更别提不同的资料对INT长度还有不同的说法，直接劝退当时纯萌新的我。现在稍微记录一下，压压惊。



据传在16位处理器的年代，INT只有2字节——即16位，对应处理器位数。

到了32位处理器的年代，INT变成了4字节——32位，依旧对应处理器位数。

> 32位处理器时代对这个世界的改变和影响终究远胜过那个16位处理器时代。
>
> 1979年8086（16位处理器）发布并流行，到1985年80386（32位处理器）发布并流行，仅仅经过了六年，而且还比较小众，所以标准比较容易变更。
>
> 而到了二十一世纪，64位处理器才逐渐流行，而32位的历史地位使得64位时代的一些规范需要向他兼容。

因此64位处理器的INT数据类型依旧是4字节，LONG为8字节——LONG本来应该是那个64位处理器的INT。在32位处理器上没有LONG，或者说LONG和INT一样都是4字节。

当然，CHAR就没那么多弯弯绕绕，从ASCII码表（当然ASCII码表只用了7bit）开始就一直老老实实的8bit，即1字节。

FLOAT也一直是4个字节，和INT一样。DOUBLE看名字就知道是double了。





# BCD码

BCD，Binary-Coded Decimal‎

编码的唯一性是最基础，在此基础上，我们还可能有一些其他的需求：

* Mechanism
* Efficiency
* Reliability
* Security



从信息论的角度来说，显然4bit能为16个事物编码，而0-9只有十种情况。但没关系，我们壕，就是要用4个bit表示一个0-9的数

## 有权码

* 8421码
  * 就像正常的INT，取0000-1001这十个码位分别表示0-9。
* 2421码
  * 从高位到低位权值分别为5、4、2、1
  * 两个2421码相加，得到的数该进位进位，剩下的部分依旧落在十个编码范围内。
    * 如5+7，1011+1101进位后剩下1000，对应的就是2的编码
  * 这是8421码无法做到的，如8421码的7+5，0111+0101=1100，而1100不在8421码的编码范围内
* 5211码
  * 从高位到地位权值分布为2、4、2、1

## 无权码

* 余3码
  * 8412码+3
  * 对9的自补码，即执行十位数相加时可以产生正确的进位信号
  * 当不产生进位数时，相加结果应该减3，产生进位数时，相加结果的剩余部分应该加3
* 余3循环码
* 格雷码
  * 0000、0001、0011、0010、0110、0111、0101、0100、……、1011、1001、1000
  * 特点：从0到15再进位回到0，每两个相邻数之间只有一个比特位变化

>  在数字系统中，常要求代码按一定顺序变化。例如，按自然数递增计数，若采用8421码，则数0111变到1000时四位均要变化，而在实际电路中，4位的变化不可能绝对同时发生，则计数中可能出现短暂的其它代码（1100、1111等）。在特定情况下可能导致电路状态错误或输入错误。使用格雷码可以避免这种错误。





[^1]:《Computer System: A Programmer's Perspective》
[^2]:https://www.bilibili.com/video/BV16x411i7FW?p=2
[^3]:https://blog.csdn.net/u010983881/article/details/79098334