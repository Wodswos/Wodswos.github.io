

# 基本信息

## 决策树与 if-then 规则



## 决策树与条件概率分布





## 决策树的构建过程

决策树不断根据各种条件判断减小信息熵，直到信息熵为零或小于某一阈值

![](C:\Users\Five\Desktop\note\img\DecisionTreeStep.png)



# 信息的度量和算法实现

案例数据

![data_cases1](C:\Users\Five\Desktop\note\img\data_cases1.png)



## 信息熵和 ID3

信息熵：确定答案或做出判断所面临的不确定性（需要的信息量）
$$
H(X) = - \sum _{i=1}^n  p_i \log_2p_i
$$
故判断某个条件 A 的信息量只需用「原本所需的信息量 」减去「在条件 A 的情况下判断事件还需要的信息量」即可
$$
H (A) = H (X|A) - H (P)
$$

n = 3 时（x = p(a), y = p(b), p(c) = 1-x-y），信息熵随概率分布的变化示意图

![image-20221108092836635](C:\Users\Five\Desktop\note\img\image-20221108092836635.png)



### 推导 & 详解

见信息论相应目录。



### 实例

如案例中 buy_computer 的信息熵
$$
H \mathrm{(buy\_computer)} = - \sum _{i=1} ^n p_i \log_2 p_i = - \frac 5 {14} \log _2 \frac 5 {14} - \frac 9 {14} \log _2 \frac 9 {14} \approx  0.940285 \mathrm{ bit }
$$
已知 age 的情况下 buy_computer 的信息熵
$$
\begin{align}
H \mathrm {(buy\_computer | age)} & = - \sum_{i=1} p_i \log_2 p_i \\
& =\frac 5{14} \times ( -\frac 2 5 \log_2 \frac 2 5 - \frac 3 5 \log_2 \frac 3 5) + \dots + \frac 5 \times (-\frac 3 5 \log_2 \frac 3 5 - \frac 2 5 \log_2 \frac 2 5) \\
& \approx 0.694 \mathrm {bit}
\end{align}
$$

信息量即为 0.246 bit



### 存在的问题

* 没有考虑连续特征（长度、密度）
  * 取值比较多的特征比取值少的特征信息增益大(哪怕两个特征可能都没有提供信息)

  * 但在sklearn提供的工具里又默认二叉树，算不算某种程度避免这一缺陷？

* 对于缺失值没有考虑

* 没有考虑过拟合

* 算法复杂度较高，可规模性一般

## Gain Rate 和 C4.5

单个离散型数据转化成多个变量，每个变量只有0/1赋值（sklearn默认二叉树）

![image](https://cdn.nlark.com/yuque/0/2021/png/21594955/1621319223196-cabc841b-3384-4615-a365-3d59a7b80f00.png)



C4.5（ID3的改进），通过函数将连续变量映射到离散值解决了不能处理连续特征的问题，

1. 取m-1个划分点(每两个相邻值的均值)
2. 分别计算以该点作为二元分类点时的信息增益
3. 选择信息增益最大的点作为该连续特征的二元离散分类点



通过采用信息增益率（信息增益/特征熵）作为标准，改善信息增益作节点选择标准容易偏爱取值较多的特征的情况
$$
H_A (D) = - \sum_{i = 1} ^n \frac {|D_i|} {|D|} \log _2 \frac {|D_i|} {|D|}
$$
特征数越多，特征熵越大，作分母用于校正



过拟合问题：引入正则化系数进行初步的剪枝，在cart中进一步优化





### 存在的问题

* C4.5生成多叉树，在计算机模型中二叉树的效率往往会优于多叉树
* 只能用于分类不能回归
* 使用熵模型，对数计算耗时



## Gini不纯度和 CART

基尼不纯度

$$
\sum _{i = 1} ^m f_i(1-f_i) = 1- \sum _{i=1} ^m f^2 _i
$$



（对于数学好的，看基尼系数的极值不过是个简单的不等式问题，但对我这种不好的，可视化一下也未尝不可）

n = 3 时的基尼不纯度值随 a , b , c 概率分布的变化图象（由概率论可知 z = 1- a -b）

（其中两个平面为 $a = \frac 1 3, b = \frac 1 3$，显然在平面的交点处取极值，即基尼不纯度确实能起到判别数据集“纯度”的效果）

![image-20221108091452336](C:\Users\Five\Desktop\note\img\image-20221108091452336.png)

![image-20221108091328706](C:\Users\Five\Desktop\note\img\image-20221108091328706.png)



二分类问题的基尼不纯度更为简洁（也是CART只用二叉树的原因之一）
$$
\mathrm {Gini}(p)  = 2p(1-p)
$$


优化计算难度带来的误差

![image](https://cdn.nlark.com/yuque/0/2021/jpg/21594955/1621319223500-ff5291d7-afa5-4030-90da-afee0776a4d3.jpg)



处理连续型特征的方法与C4.5类似，不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程（对半再对半）（不同于C4.5可以多叉）



## 方差和回归树

* 与分类树不同的地方
* 连续值处理方法不同

不再用基尼不纯度，而是采用和方差的度量方式

对于任意的划分特征A，比较分类后的两个集合的均方差之和，找到划分后均方差之和最小的特征

$$
\min [\sum _{x_i \in D_1} (y_i - c_1) ^2 + \sum_{x_i \in D_2} (y_i - c_2)^2]
$$


其中c1,c2分别为两个划分集的均值，对于连续值，找到合适的划分点（参见C4.5连续值处理）使得均方差之和最小

进而比较各个划分特征各自的结果，取均方差最小的特征为下一个划分标准。



* 决策树建立后的预测方法不同

   CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子（叶子数据集）的均值或者中位数来预测输出结果





* 剪枝

CART回归树和CART分类树的剪枝策略除了在度量损失的时候一个使用均方差，一个使用基尼系数，算法基本完全一样

对CART树进行剪枝，类似于线性回归的正则化

* 从原始决策树生成各种剪枝后的决策树

* 剪枝的损失函数度量


$$
C_\alpha(T_t) = C(T_t) + \alpha |T_t|
$$
α为正则化参数，C(T)为训练误差，由基尼系数或均方差度量

* 正则化参数越大，剪枝程度越大
* 参数为零即不剪枝
  * 参数无穷大只剩根节点

$$
\alpha = \frac {C(T) - C(T_t)}{ |T_t|  - 1}
$$





* 交叉验证泛化能力

![image](https://cdn.nlark.com/yuque/0/2021/png/21594955/1621319223872-67823572-d697-4db7-b1b5-952d419e4b58.png)



to be continue...



多变量决策树OC1

集成学习-随机森林：改善决策树因样本微小变动而剧烈改变







# 剪枝







# 代码实现

## 手撕





## sklearn

