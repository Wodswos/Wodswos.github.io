详细内容参见数理基础部分笔记。
* [[0.0-先验知识篇]]
* [[0.1-脚手架和技巧篇]]
* [[1.1-极限-分析之本]]
* [[2.1-从导数到微分-导数的定义、计算、性质和应用]]
* [[2.2-从导数到微分-深入理解微分]]
* [[2.3-从导数到微分-多元函数的导数和微分]]
* [[3.1-积分-积分的定义、计算和基本应用]]
* ……





# 多元函数和梯度下降

> 将一个神经网络视为一个多元函数。
>
> 则多元函数的**自变量**在实际的神经网络中指的是**权重weight**，即神经元参数，而不是**训练数据**，**训练数据**是多元函数的系数（以简单的线性回归和平方损失为例）。
>
> 虽然损失函数常写作如下形式（以简单的线性回归和平方损失）
> $$
> l(y,\hat y )= \frac 1 2 (y-\hat y) ^2 \\
> 其中 \quad y = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 +b
> $$
> 但梯度下降时，自变量是那堆记为 $w$ 的变量，而不是如同一般数学习惯里把自变量记为 $x$ 。

## 多元微分和方向导数

**输入为向量，输出为标量**即高等数学中一般的多元函数。

一般输入是一个列向量，其导数为横向量。
$$
\mathbf{x}=
\begin{bmatrix}
x_1\\ x_2\\ \vdots \\ x_n
\end{bmatrix}

\qquad

\nabla y =\frac {\partial y} {\partial \mathbf x} = 
\begin{bmatrix}
\frac{\partial y} {\partial x_1},\frac{\partial y} {\partial x_2},\dots,\frac{\partial y} {\partial x_n}

\end{bmatrix}
$$
以 $y = x_1 ^2 +2x_2^2 $ 为例，其导数为 $[2x_1,4x_2]$ .



## 梯度





## 其他情况

### 输入标量，输出向量

可以大致视为多个一元函数结果的拼接，组成一个列向量。

（**这种情况似乎也谈不上梯度的说法了**，毕竟x是标量，就正负两个方向）

（事实上也一般不可能有这种情况——咋可能损失函数就跟一个神经元的weight相关联，这还叫神经网络吗）
$$
\mathbf{y}=
\begin{bmatrix}
y_1\\ y_2\\ \vdots \\ y_m
\end{bmatrix}

\qquad

\frac {\partial \mathbf y} {\partial x} = 
\begin{bmatrix}
\frac{\partial y_1} {\partial x} \\ 
\frac{\partial y_2} {\partial x} \\
\dots \\
\frac{\partial y_m} {\partial x}

\end{bmatrix}
$$




### 输入输出均为向量

这种情况可以视为多个多元函数的组合，即多个“输入向量，输出标量”的组合，将多个横向量组成一个矩阵。
$$
\mathbf{x}=
\begin{bmatrix}
x_1\\ x_2\\ \vdots \\ x_n
\end{bmatrix}

\mathbf{y}=
\begin{bmatrix}
y_1\\ y_2\\ \vdots \\ y_n
\end{bmatrix}

\qquad

\frac {\partial \mathbf y} {\partial \mathbf x} = 
\begin{bmatrix}
\frac{\partial y_1} {\partial x_1},\frac{\partial y_1} {\partial x_2},\dots,\frac{\partial y_1} {\partial x_n} \\
\frac{\partial y_2} {\partial x_1},\frac{\partial y_2} {\partial x_2},\dots,\frac{\partial y_2} {\partial x_n} \\
\vdots \\
\frac{\partial y_m} {\partial x_1},\frac{\partial y_m} {\partial x_2},\dots,\frac{\partial y_m} {\partial x_n} \\
\end{bmatrix}
$$


### 输入输出均可扩展到矩阵



## 链式求导

略




## 实例

[[1.0-万物起源，线性回归#典中典：波士顿房价预测#线性回归中的梯度和求导]]

[[1.0-万物起源，线性回归#感知机#梯度和求导]]

# 计算机求导方法[^1]

![image-20220323213849926](C:\Users\Five\Desktop\note\img\image-20220323213849926.png)

## Numerical Differentiation 和 Symbolic Differentiation

Numerical Differentiation，数值微分，存在两种误差

* 截断误差
  * 如计算 $\sin  x$ 的值，将其泰勒展开，只计算前面几项，而将后面的项截断
* 舍入误差
  * 计算机用浮点数表示实数，难免存在精度不够的情况



Symbolic Differentiation，符号微分，类似于手工计算，是计算机根据规则进行微分的方式，试图将问题转化为一个纯数学符号问题，但容易产生**表达式膨胀（expression swell）**的问题。

## 计算图和自动微分

> 自动微分 Automatic Differentiation 又称 Algorithmic differentiation或 computational differentiation.

Fundamental to AD is the decomposition of differentials provided by the [chain rule](https://en.wikipedia.org/wiki/Chain_rule). (From wikipedia)

* 将代码分解成操作子，将计算表示成一个无环图
* 显示构造：Tensorflow/Theano/MXNet
* 隐式构造：Pytorch/MXNet



### 前向和反向

正向累计，forward accumulation.
$$
\frac {\partial y } {\partial x} = 
\frac {\partial y} {\partial u_n}  
(\frac {\partial u_n} {\partial u_{n-1}}
(\frac {\partial u_{n-1}} {\partial u_{n-2}}
(\dots 
(\frac {\partial u_2} {\partial u_1}  \frac {\partial u_1} {\partial x}))))
$$
> 在自动求导中，所有的求导结果**stored as a numerical value, not a symbolic expression**

![](C:/Users/Five/Desktop/note/img/ForwardAccumulationAutomaticDifferentiation.png)



反向累计，reverse accumulation.
$$
\frac {\partial y } {\partial x} =
 ((( \frac {\partial y} {\partial u_n}  
 \frac {\partial u_n} {\partial u_{n-1}}) 
 \dots) 
 \frac {\partial u_2} {\partial u_1}) \frac {\partial u_1} {\partial x}
$$

![](C:/Users/Five/Desktop/note/img/ReverseaccumulationAD.png)

> 正向累计相比于反向累计会更直观。

> 形如 $\frac {\partial y} {\partial u_n}$ 的微分都是在后续的计算中被需要的，而形如 $\frac {\partial u_n} {\partial x}$ 的微分（y对x的微分除外）在后续的计算中不被需要。
>
> 故反向累计的方法可以较好地减少计算冗余。

> 微分流形上的矢量有两种：切空间 (Tangent space)的矢量和对偶空间 (Dual space)的矢量。
> 其中切空间的基是关于坐标系的偏导,对偶空间的基是关于坐标系的微分。从切矢量出发可以得到自动微分的正序模式 (forward mode, tangent-linear)，从对偶矢量出发可以得到自动微分的逆序模式 (reverse mode，adjoint mode,backward mode)。
> ————————————————
> 版权声明：本文为CSDN博主「DanielWang_」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
> 原文链接：https://blog.csdn.net/daniel_ustc/article/details/77133329

> 当输出的维度大于输入的时候，适宜使用前向模式微分；当输出维度远远小于输入的时候，适宜使用反向模式微分。

## 自动求导的实现

### 代码转换





### 运算符重载





[^1]: Automatic Differentiation in Machine Learning: a Survey,  arXiv:1502.05767v4 [cs.SC] 5 Feb 2018
[^2]:http://www.autodiff.org/
[^3]:https://en.wikipedia.org/wiki/Automatic_differentiation
[^4]:Calculus on Computational Graphs: Backpropagation http://colah.github.io/posts/2015-08-Backprop/
[^5]:自动微分法（Automatic differentiation）是如何用C++实现的？ - 李瞬生的回答 - 知乎 https://www.zhihu.com/question/48356514/answer/123290631
