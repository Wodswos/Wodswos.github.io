[[d2l-zh-pytorch.pdf]]

# 注意力提示

> 注意力经济时代：人的注意力被视为可交换的、有限的、有价值的且稀缺的商品。
>
> 在音乐或视频流媒体服务上，我们要么消耗注意力在广告上，要么付钱来隐藏广告；在网络游戏世界同理。[^1]

## 神经认知科学领域

### 非自主性提示和自主性提示







### 具体场景

高考英语的七选五



估算自己进入一个公司能够得到的薪水，最粗暴的方法是取公司的平均值，但这显然是不合理的，更合理的做法是考虑能力相近、工作相近的人的薪水（给那些人更大的权重/注意力分数）。







## Attention vs xxx



### Attention vs 一维卷积

attention跟一维卷积的区别是啥？ - 莫驚蟄的回答 - 知乎 https://www.zhihu.com/question/288081659/answer/1222002868

![](C:/Users/Five/Desktop/note/img/v2-1d3b534e156f08fd762cd1e669308425_720w.jpg)

* 卷积需要指定窗口大小，所以相对而言卷积是 **local** 的，而 attention 权重的计算涉及一个序列中的所有的词，相对而言是 **global** 的。
* （推理阶段）卷积的连接权重是与输入无关的，注意力机制的连接强度是和输入相关的。



### Attention vs 全连接层

深度学习中Attention与全连接层的区别何在？ - SleepyBag的回答 - 知乎 https://www.zhihu.com/question/320174043/answer/651998472

Attention的最终输出可以看成是一个“在关注部分权重更大的全连接层”。但是它与全连接层的区别在于，注意力机制可以利用输入的特征信息来确定哪些部分更重要。

**注意力机制的意义是引入了权重函数f，使得权重与输入相关，从而避免了全连接层中权重固定的问题**

**全连接指的是从一个特征空间到另一个特征空间的映射，而注意力机制是要对来自同一个特征空间的多个实体进行整合。**





# 注意力汇聚（池化）

## 非参：Nadaraya-Waston核回归







## 参数化的注意力机制







# 注意力评分函数





# Bahdanau 注意力





# 多头注意力







# 自注意力

![image-20220526134559208](C:\Users\Five\Desktop\note\img\image-20220526134559208.png)





## 位置编码

跟CNN/RNN不同，自注意力并没有记录位置信息。

位置编码将位置信息注入到输入里。![image-20220526145538248](C:\Users\Five\Desktop\note\img\image-20220526145538248.png)







[^1]: 《Dive into deep learning》