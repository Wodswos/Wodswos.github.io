> 香农通过《A Mathematical Theory of Communication》开创了信息论，严格度量了一个物理量（信息），给出了一个全新的认知世界的视角。
>
> 香农的硕士论文与信息论关系不大，但也是极有意义的工作：继电器与开关电路的符号分析（也许可以理解为 A Mathematical Theory of relay switch circuit？）。
>
> 从中可以看到香农有很强的从复杂工程（通信、电路）中提取通用抽象逻辑（Mathematical Theory）天赋和能力。
>
> 后续还有《A Mathematical Theory of Cryptography》开辟了用信息论来研究密码学的新思路。
>
> 
>
> 十年前，香农将电路开关抽象成0和1，让电路的设计工作变成了利用布尔代数求解的问题；十年后，香农又把信息抽象成了0和1，让改进通讯质量的工作也变成了求解数学题。从这可以看到香农超强的抽象能力，它能够抽丝剥茧，将万千信息的表象过滤掉，直接找到信息的本质，用数学的方式表示出来，并且能够度量大小。
>
> 香农的信息论究竟牛在哪里？ - 苗华栋的回答 - 知乎 https://www.zhihu.com/question/27068465/answer/1367210750

# 熵、不确定性和信息



## 不确定性的度量

> 如果看字面意思的话，无序性或者混乱度指的是系统的某种「排列」有多么混乱，有多么无序。可是谁来规定哪种排列更无序呢？这就是无法规定的。而使用这两个词的人，想表达的意思其实是系统的「不确定性」。
>
> 因此，我希望大家能够别用「无序性」、「混乱度」，而是改用「不确定性」。
>
> 熵与不确定性 - 会钟意的文章 - 知乎 https://zhuanlan.zhihu.com/p/380224743

* 不确定性 $H$ 应该完全由概率分布 $\{p_i\}$ 描述，且不确定性应关于 $p_j$ 连续
* 不确定性 $H$ 最小的概率分布应该是某个 $p_i = 1$. 即此时没有不确定性。
* 不确定性 $H$ 最大的概率分布应该是所有的 $p_i = \frac 1 \Omega$，即所有可能情况等概率。
* 复合系统的不确定性应为各个子系统不确定性的加权和，即 $H = p_1H_1 + p_2H_2$



最后发现，满足这些条件的，有且只有如下形式（不提供证明，因为不会，也没看香农原文）：
$$
H(X) = -k\sum _{i=1} p_i \log p_i \\
$$

即对于一个离散型随机变量 $X \sim p(x)$，其离散熵可以定义为：
$$
H(X) = - \sum _{i=1} p_i \log_2 p_i
$$

> 理论上熵中的对数函数可以采用任何底数，并且采用不同底数，其对应的单位也不同。
>
> 熵中如采用 2 为底数（即 $\log _2$ ），则对应的单位为比特（bit）；如果采用 3 为底数（即 $\log _3$），则对应的单位为Tet，如果采用 e 为底数（即 $\ln$），对应单位为奈特（nat）；采用 10 为底数（即 $\lg$），则对应单位为Hart。
>
> 为什么信息熵的底数是2？ - 李sir的回答 - 知乎 https://www.zhihu.com/question/514755014/answer/2334948044

> Counting bits, information theory tends to be interested in how many bits of information are involved. Other bases will be ok too.
>
> 为什么信息熵的底数是2？ - 搞AI的哈士奇的回答 - 知乎 https://www.zhihu.com/question/514755014/answer/2335345722

> 熵是平均意义上对随机变量的编码长度（熵的计算以 k 为底数，编码系统也以 k 为进制，通常 k = 2）

### 热力学的熵

对于热力学系统，显然 $p_i$ 均相等，$k$ 为玻尔兹曼常数
$$
H = -k_B \sum _{i=1} ^n p_i \ln p_i = -k_B \sum _{i=1} ^n \ln p_i \\

S = k \ln \Omega
$$

先略，以后可能填坑。

### 条件熵

很显然，熵的度量就是基于概率论中的概率分布而来。在概率论中有条件概率的概念，将其应用于信息论中，就可以得到条件熵的概念。
$$
H(Y|X) = \sum _{i=1} ^n p(x_i) H(Y|X=x_i)
$$

### 互信息

$$
I(X;Y) = H(Y) - H(Y|X)
$$

也可以看成 X 给 Y 带来的信息增益。互信息这个名称在通信领域经常使用，信息增益则在机器学习领域中经常使用。

## 信息：消除不确定性

> 举个例子，「中国男子足球队获得世界杯冠军」的信息显然要比「中国男子乒乓球队获得世界杯冠军」的信息要大得多。
>
> 究其原因，国足勇夺世界杯相对于国乒是如假包换的小概率事件（如果不是不可能事件的话）。
>
> —— 极客时间 人工智能基础课 王天一

显然，信息量即「加入信息前的概率分布不确定性」和「加入信息后的概率分布不确定性」之差。



假设概率均为 $\frac 1 n$，则对二分这一行为提供的信息：
$$
H(Z_2) -H(Z_1) = -  n \log_2 \frac 1{n} - (- \frac n 2  \log_2 \frac 2 n)  \\
=-n \log_2 \frac 1 n - \frac n 2  (1 +\log_2 \frac 1 n)) \\
=\frac n 2 - \frac n 2 \log_2 \frac 1 n
$$


# 香农三大定理

香农的信息论究竟牛在哪里？ - 科言君的回答 - 知乎 https://www.zhihu.com/question/27068465/answer/96502561

1）临界数据压缩的值？答案：信息熵H。实用的编码平均码长的理论下界就是信息熵。

2）临界通信速率的值？答案：信道容量C。







如何连贯地理解香农三大定理？ - BeyondSelf的回答 - 知乎 https://www.zhihu.com/question/39296849/answer/1467262363

![](C:/Users/Five/Desktop/note/img/v2-e1ae0c48e065069fc31d5fed142ef681_720w.webp)




$$
C = W \log (1 + \frac P N)
$$



# 经典问题

通用步骤：

* 明确「问题对应的样本空间」、「能带来信息的（往往是独立的）随机实验」以及每次实验能带来多少「信息」（随机试验可能出现的情况/状态）
  * 在毒水和猪的问题里，样本空间是1000桶水，随机试验是能喝4次水机会的猪，带来信息 $\log _2 5 \approx 2.322 \mathrm{bit}$（ 5 为猪的 5 种状态） 
  * 在天平称球的问题里，样本空间是 N 个球，随机试验是天平称重，带来信息 $\log_2 3\approx 1.585 \mathrm{bit}$（ 3 为称重的 3 种可能结果/状态） 
* 以状态数作为进制，给样本空间内的「每一个可能性（elements）」编号
* 细化实验，使得每个独立实验都能确定目标 elements 在某个「位」上的值。

## 毒水和猪

1000桶水，其中一桶有毒，猪喝毒水后会在15分钟内死去，想用一个小时找到这桶毒水，至少需要几头猪？ - 苗华栋的回答 - 知乎 https://www.zhihu.com/question/60227816/answer/1274071217



## 天平称球

> 香农的信息论究竟牛在哪里？ - 苗华栋的回答 - 知乎 https://www.zhihu.com/question/27068465/answer/1367210750

> 有 N 个小球，其中存在且仅存在（这也是一个很强的条件）一个小球的重量更轻（或更重），现有一台没有砝码的天平，问至少称几次能确保找出这个特殊的小球。

一个天平称量1次后会有3种结果：

1. 左轻右重
2. 左重右轻
3. 左右一样重

即一次称量带来的信息（减少的信息熵/不确定性）为：
$$
H(Z) = -(\frac 1 3 \log_2 \frac 1 3 + \frac 1 3 \log _2 \frac 1 3 + \frac 1 3 \log \frac 1 3) = - \log_2 \frac 1 3 = 1.585
$$

以 N 取 12 为例，此时的信息熵为：
$$
H(X) = -\log_2 \frac 1 {12} = 3.585
$$




### 只知不同，但不知轻还是重的球

知乎高赞给的思路是，「样本空间里找一个 element」这个随机变量X的信息熵增加了，变成 $H(X) = -\log_2 \frac 1 {24} = 4.585$，但「称量实验」这个随机变量Y的信息熵没有变。



> 个人一开始的想法是，「样本空间里找一个 element」的信息熵还是没变，但「称量实验」能带来的信息变了，只有在称上和不在称上两种情况，即变成了 $H(Y) = \log _2 2 = 1 \mathrm{bit}$。但这样实际上丢失了信息。 





# 应用之编码

编码导致了通用图灵机（Universal Turing Machines）概念的产生——一个能够输入程序和数据，并在数据上运行程序的机器。[^1]





## 压缩

> 瓜 保熟；篮球 背带裤；
>
> 什么叫视频的极限压缩啊。



# 应用之分类特征的选择



利用信息增益的概念可以选择分类的特征，显然，信息增益更大的特征具有更强的分类能力。



信息增益比
$$
g(X,Y) = \frac {I(X;Y)} {H(Y)}
$$

## KL 散度

Kullback-Leibler 散度
$$
D_{KL} (P||Q) = \sum _{i=1} ^ n p(x_i) \log_2 \frac {p(x_i)}{q(x_i)}
$$
KL 散度有非负性和非对称性。



# 学习资料 & 路径规划

* [ ] 《信息熵 理论与应用》
* [ ] 《信息论与编码》田枫





* [ ] zip炸弹
* [ ] 世界上最大的文件压缩率是多少？ - 何先森饭扫光的回答 - 知乎 https://www.zhihu.com/question/20583075/answer/26673424
* [ ] 香农的信息论究竟牛在哪里？ - 科言君的回答 - 知乎 https://www.zhihu.com/question/27068465/answer/96502561
* [ ] 如何连贯地理解香农三大定理？ - BeyondSelf的回答 - 知乎 https://www.zhihu.com/question/39296849/answer/1467262363
* [ ] 



[^1]:《论可计算数：图灵与现代计算的诞生》[美] Chris Bernhardt 著，雪曼 译