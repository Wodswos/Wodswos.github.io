# 模型的训练

## 训练的一般流程

```
定义并初始化网络
for epoch:
	for batch:
		前向计算
		反向传播
# 一般采取批量化的策略，将小批量上的loss视为整个训练集的loss
```

对于RNN，每个batch其实有 batch_size 乘以 time_steps 个分类样本。（time_steps的地位有点类似于CNN的image_size）

### 一些常见的缩写



## 验证数据集和测试数据集

验证（validation）数据集：一个用来评估模型好坏，进而再调整超参数或者模型其他方面的数据集。

就像是**模拟考试**。

**测试数据集一般来说是仅用一次的数据集**，就像是**高考**——只能用于最终的评分，而不会用于日常训练。

总之，**对于一次测试，需要保证模型在此之前没见过这些测试数据**，测试才有意义、有说服力（排除了模型overfitting了训练集的可能性）。

## k-折交叉验证

-   将训练数据分割为 K 块
    
-   For i = 1, ... , K
    
    -   使用第 i 块作为验证数据集（其余为训练数据集）
    
-   报告 K 个验证集的平均误差。
    

常用 K=5 或者 K=10。





# 过拟合和欠拟合





# 过拟合的处理

> 应对过拟合最有效的办法也许是扩大有效样本。但很遗憾，绝大部分时候，扩大有效样本这点是很难做到的，因此只能在已有的样本上，略施巧计，使得过拟合的情况得到缓解。
>
> 也可以在原始数据上做些改动，得到更多的数据。

## 权重衰减和L2正则化

在使用朴素的梯度下降法时**权重衰减**和**L2正则化**二者是同一个东西，因为此时L2正则化的正则项对梯度的影响就是每次使得权值衰减一定的比例。

### 硬性限制

$$
\min l(\mathbf w ,b) \quad 
\mathrm {subject \ to} \quad \Vert \mathbf w \Vert ^2 \leqslant \theta
$$

更小的 $\theta$ 意味着更强的正则项。



### 柔性限制

对于每个 $\theta$ ，都可以找到 $\lambda$ 使得之前的目标函数等价于下面的目标函数
$$
\min [l(\mathbf w,b) + \frac \lambda 2 \Vert \mathbf w \Vert ^2]
$$

> 可以通过拉格朗日乘子证明。

![image-20220325161233102](C:\Users\Five\Desktop\note\img\image-20220325161233102.png)
$$
\frac {\partial}{\partial \mathbf w} (l(\mathbf w,b) + \frac \lambda 2 \Vert \mathbf w \Vert ^2) = \frac {\partial l(\mathbf w,b)}{\partial \mathbf w} + \lambda \mathbf w 

\\
\mathbf w _{t+1} =\mathbf w + (\nabla _1 + \nabla _2) \eta = (1- \eta \lambda) \mathbf w _t - \eta \nabla_1
$$
而通常 $0 < \eta \lambda < 1$ ，若不考虑原损失函数的梯度影响，权重是衰减的，故成为权重衰减。



> 权重衰退通过L2正则项使得模型参数不会过大，从而控制模型复杂度。

### 为什么控制权重的取值范围可以缓解过拟合，即为什么权重衰减有效



## L0和L1正则化







## Dropout

> * Dropout 将一些输出项随机置为0来控制模型复杂度
> * 常作用在多层感知机的隐藏层输出上
> * 丢弃概率常取 0.1/0.5/0.9

* $\mathbf x$ 是某一层到下一层之间的输出，如 $\mathbf {x_2} =\sigma(\mathbf {W_2x_1 + b_1})$）
* 对 $\mathbf x$ 加入噪音（即 $\mathbf {x'} = \mathrm {dropout} (\mathbf x)$）得到 $\mathbf {x'}$ 
* 尽管加入噪音，但依旧希望 $E[\mathbf {x'}] = x$ ，从而有

$$
x'_i = 
\begin{cases}
0     \quad \mathrm {with \  probability \ } p \\
\frac {x_i} {1-p}      \quad \mathrm {otherwise}
\end{cases}
$$



通常将 Dropout 用在隐藏全连接层的输出上。

> * 正则项（包括之前的L2正则和Dropout）**只在训练中使用**：影响模型**参数的更新**
> * 在推理过程中Dropout并不产生作用，即 $\mathbf h = \mathrm {dropout} (\mathbf h)$
>   * 保证确定性的输出

### Why dropout work

* Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: A simple way to prevent neural networks from overfitting[J]. The Journal of Machine Learning Research, 2014, 15(1): 1929-1958.

* Dropout as data augmentation. http://arxiv.org/abs/1506.08700

#### 组合派，动机论

也是Hinton（最初）的观点：每一次dropout都是从神经网络中抽取出一个子神经网络，最终进行ensemble。



某种程度上认为dropout类似于动植物大多使用有性生殖的意义



#### 噪声派

dropout提出后大家用着用着，觉得从实验上来说dropout和正则效果是一样的。





# 模型的稳定性

对于一个共有n层的神经网络，第k层的梯度需求经过 n-k 次矩阵乘法得到。

而极端情况下，如 $1.5^{100} \approx 4\times 10^{17}$ ，就会面临梯度爆炸，$0.8^{100} = 2 \times 10^{-10}$，即梯度消失

## 梯度爆炸

使用  ReLU 作为激活函数，ReLU 的导数较为简单，在大于0时为1，小于零时为0. 故最终的梯度主要由参数决定。



梯度爆炸的问题

* 值超出值域（对于16位浮点数尤为严重）
* 对学习率敏感
  * 学习率大导致恶性循环：更新步长大 -> 参数值大 -> 梯度大（对于ReLU而言梯度主要由参数决定）
  * 学习率小，则进展缓慢，甚至无进展



## 梯度消失

使用 sigmoid 作为激活函数，
$$
\sigma (x) =\frac 1 {1 + e^{-x}}  \quad \sigma'(x)= \sigma(x)(1-\sigma(x))
$$
即意味着输入的绝对值很大时， sigmoid 的梯度会很小。





梯度消失的问题

* 梯度值变为零（对16位浮点数尤为严重）
  * 此时显然已经无法继续训练
* 更底部的网络显然更容易梯度消失，即意味着无法让网络加深。



## 挽救措施

让梯度值在合理的范围内。

### 变乘法为加法

ResNet，LSTM  

### 归一化





### 合理的权重初始和激活函数

