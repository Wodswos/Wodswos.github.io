# 序列模型和基本思路

* 之前一般默认数据都来⾃于某种分布，并且所有样本都是独⽴同分布的，但实际中很多数据是有时序结构的
  * 如电影的评价随时间的变化而变化，同样一句话在不同的语境中有完全不同的意思。
* 其次，我们不仅仅可以接收⼀个序列作为输⼊，⽽是还可能期望继续猜测这个序列的后续



传统的前馈神经网络只能单独地处理一个个信息，即从概率上来讲，前一个输入和后一个输入是互相独立的。

但是在处理序列模型（如试图理解一句话的意思）时，显然不能将每一个信息孤立地看待，而应该将其视为一个完整序列的一部分去处理。

> 简言之，如果说卷积神经⽹络可以有效地处理空间信息，那么本章的循环神经网络（recurrent neural network， RNN）则可以更好地处理序列信息。
>
> 循环神经⽹络通过引⼊状态变量存储过去的信息和当前的输⼊，从而以确定当前的输出。[^2]

## 用统计工具建模

在时间 $t$ 内观察到 $x_t$，那么得到 $T$ 个不独立的随机变量 $(x_1,x_2,\dots,x_T) \sim p(\mathbf x)$.

使用条件概率（$p(a,b) = p(a) p (a|b) = p(b) p(b|a)$）展开
$$
p(\mathbf x) = p(x_1) \cdot p(x_2 | x_1) \cdot p(x_3 |x_1,x_2) \cdots p(x_T|x_1,\dots,x_{T-1}) \\
p(\mathbf x) = p(x_T) \cdot p(x_{T-1} | x_T) \cdot p(x_{T-2} |x_T,x_{T-1}) \cdots p(x_1|x_T,\dots,x_2)
$$
**对条件概率建模：**
$$
p(x_t|x_1,\dots,x_{t-1}) = p(x_t | f(x_1,\dots,x_{t-1}))
$$
当然可以用回归模型去拟合，即对见过的数据建模，称为**自回归模型**（Auto Regression）。

### 马尔可夫假设

但随着时间 t 的增长，输入变量会无限制增长，这是比较难以承受的。

故我们需要一个方法估算 $P(x_t|x_1,\cdots,x_{t-1})$，使得**计算得到足够简化**的同时，**估算结果也足够精确**。



* 策略1（马尔可夫假设）：假设只需要满足某个长度为 $\tau$ 的时间跨度，即使用观测序列 $x_{t-1},\dots,x_{t-\tau}$
  * 基于此可以建立并训练深度网络
* 策略2（潜变量模型）：用一个变量 $h_t$ 作为过去观测的总结。
  * 即 $h_t = f(x_1,\dots,x_{t-1})$，于是有 $x_t = p(x_t|h_t)$
  * 也称为潜变量（隐变量）自回归模型，即RNN的工作原理
    * 隐（Hidden）变量一般是某个没有观察到的真实存在的量，而潜（Latent）变量的意义更广，可能是某个现实意义上不存在的量
    * 潜变量模型一般也可以借鉴马尔科夫假设
  * ![image-20220426163941022](C:\Users\Five\Desktop\note\img\image-20220426163941022.png)








### 因果关系

在通常情况下，解释 $p(x_t | x_{t-1})$ 要比解释 $p(x_t | x_{t+1})$ 容易得多，甚至后者可能在物理上就不被允许。





### 训练和预测

![image-20220426163832276](C:\Users\Five\Desktop\note\img\image-20220426163832276.png)



# 循环神经网络及其简单实现

## RNN发展史

1933年，西班牙神经生物学家 Rafael Lorente de No 发现大脑皮层的解剖结构允许刺激在神经回路中循环传递，并由此提出反响回路假设。

1982年，美国学者John Hopfield 基于 Little 的神经数学模型使用二元节点建立了具有结合存储能力的神经网络，即 Hopfield 神经网络。

1986年，Michael I. Jordan （Bengio的导师）在分布式并行处理理论下提出了 Jordan网络。

> Jordan 网络的每个隐含层节点都与一个状态单元相连以实现延时输入，并使用 logistic 函数作为激励函数。
>
> Jordan 网络使用Back-propagation 进行学习。

1990年，Jeffrey Elman提出了第一个全连接的RNN，即Elman网络。Jordan网络和Elman网络都**从单层前馈网络出发**构建递归链接，也被称为**简单循环网络（SRN**）。

> 同一时期，RNN的学习理论也得到了发展。在BP算法被提出后，学界开始尝试在BP框架下对循环神经网络进行训练。
>
> 1989年，Ronald Williams和David Zipser提出了RNN的实时循环学习。
>
> 1990年，Paul Webos在1990年提出了随时间反向传播算法（BP Through Time，BPTT）

1991年，Sepp Hochreiter发现了循环神经网络的长期依赖问题：在对长序列进行学习时，循环神经网络会出现梯度消失和梯度爆炸的现象，无法掌握长时间跨度的非线性关系。

**1992和1997年，Jurgen Schmidhuber及其合作者提出了神经历史压缩器（Neural History Compresssor，NHC）和长短期记忆网络（Long Short-Term Memory Networks，LSTM）（截至2022有6W4的引用）。**

**同年（1997），M. Schuster 和 K. Paliwal 提出了具有深度结构的双向循环神经网络，并进行了语音识别实验。**

随着深度学习理论的出现和数值计算能力的提升，拥有更高复杂度的RNN开始在自然语言处理问题中得到关注。

**2013-2015年，Y. Benjo、D. Bahdanau等提出了Encoder-Decoder、自注意力层等一系列RNN算法，并将其用于机器翻译问题。在随后的研究中启发了包括Transformers、XLNet、ELMo、BERT等复杂构筑。**



## 潜变量自回归模型，aka 循环神经网络

无潜变量的神经网络（多层感知机）：
$$
H = \phi(XW_{xh} + b) \\
O = HW_{hq} + b_q
$$
有潜变量的神经网络（循环神经网络）：
$$
H_t = \phi(W_{hh}H_{t-1} + W_{hx}X_{t} + b_h) \\
O_t = H_tW_{hq} + b_q
$$

> 隐状态中 $X_tW_{xh}+H_{t-1}W_{hh}$ 的计算，可以视为 $X_t$ 与 $H_{t-1}$ 的拼接和 $W_{xh}$ 与 $W_hh$ 的拼接的矩阵乘法。
>
> 即对于：
>
> ```python
> X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4)) 
> H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))
> ```
>
> 以下两种计算是等价的：
>
> ```python
> torch.matmul(X, W_xh) + torch.matmul(H, W_hh)
> torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))
> ```



### RNN的图示和简化图示

下图[^4]左侧的RNN可以简化为右侧的图示（一个网络层简化为一个点，同时时间轴从纵向变成横向）：

![image-20220427111937160](C:\Users\Five\Desktop\note\img\image-20220427111937160.png)

《Dive into Deep Learning》一书和Colah的博客文章里都常用这种图示

![image-20220426184121986](C:\Users\Five\Desktop\note\img\image-20220426184121986.png)

![](C:/Users/Five/Desktop/note/img/RNN-unrolled.png)

也可以进一步简化为下图右侧的图示：

![image-20220427111821280](C:\Users\Five\Desktop\note\img\image-20220427111821280.png)



### 基于RNN的字符级语言模型

即词元为字符，batch_size设为1的情况。取文本序列为"machine"

![image-20220427161738872](C:\Users\Five\Desktop\note\img\image-20220427161738872.png)



### 损失函数和困惑度

因为输出一般是分类（有限个词元），故一般对输出层进行softmax操作。

softmax 做输出层的激活函数配交叉熵作损失函数也是经典组合了。
$$
\pi = \frac 1 n \sum ^{n} _{t=1} -\log p(x_t|x_{t-1},\dots)
$$
历史原因 NLP 用困惑度 $e^{\pi}$ 来衡量模型



### 梯度剪裁

迭代中计算 $T$ 个时间步上的梯度，在反向传播过程中产生长度为 $O(T)$ 的矩阵乘法链，导致数值不稳定。





### RNNs

![image-20220427170409552](C:\Users\Five\Desktop\note\img\image-20220427170409552.png)



##  简单实现







## 关于递归神经网络

如Hopfield神经网络，略



# 门控循环单元GRU

Gated Recurrent Unit，门控循环单元。

**GRU的提出要比LSTM晚，或者说就是LSTM的一种变体，比LSTM更简单，但效果不差。**

## 问题和背景

* 早期的观测值可能对预测所有未来的观测值具有非常重要的意义
* 一些词元没有相关的观测值
  * 例如，在对网页内容进行情感分析时，可能有一些辅助HTML代码与王爷传达的情绪无关。
* 序列的各个部分之间存在逻辑中断。



## GRU隐状态更新过程

### 门

$$
R_t = \sigma (X_tW_{xr} + H_{t-1}W_{hr} + b_r) \\
Z_t = \sigma (X_tW_{xz} + H_{t-1}W_{hz} + b_z)
$$

$R_t$ 衡量要不要遗忘过去的信息，$Z_t$ 衡量要不要更新指隐状态

![image-20220429182136820](C:\Users\Five\Desktop\note\img\image-20220429182136820.png)



### 候选隐状态

$$
\tilde {H} = \tanh (X_t W_{xh} + (R_t \odot H_{t-1}) W_{hh} + b_h )
$$

（如果不考虑上式中的 $R_t$，则上式中的 $\tilde H$ 就是最基本的RNN模型的隐变量更新值）

![image-20220429182159147](C:\Users\Five\Desktop\note\img\image-20220429182159147.png)



### 最终隐状态更新

$$
\tilde H _t = Z_t \odot H_{t-1} + (1-Z_t) \odot \tilde H _t
$$

极端情况下，$Z_t = 0$ ，Reset Gate也没有作用的时候，就回到了最朴素的RNN的情形。

![image-20220429184239744](C:\Users\Five\Desktop\note\img\image-20220429184239744.png)



# 长短期记忆LSTM

1997年的“老古董”了。

> Essential to these successes is the use of “LSTMs,” a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It’s these LSTMs that this essay will explore.
>
> http://colah.github.io/posts/2015-08-Understanding-LSTMs/

LSTM，Long Short-Term Memory，一种时间循环神经网络，是为了解决一般的RNN存在的长期依赖问题而专门设计出来的。

由于独特的设计结构，LSTM适合于处理和预测时间序列中间隔和延迟非常长的重要事件。



## 奇怪的设计

* 遗忘门：将值朝0减少
* 输入门：决定是不是忽略掉输入数据
* 输出门：决定是不是使用隐状态


$$
I_t = \sigma(X_t W_{xi} + H_{t-1}W_{hi} + b_i) \\
F_t = \sigma(X_t W_{xf} + H_{t-1}W_{hf} + b_f) \\
O_t = \sigma(X_t W_{xo} + H_{t-1}W_{ho} + b_f) \\ \\

\tilde C = \tanh (X_tW_{xc} + H_{t-1}W_{hc} + b_c) \\
C_t = F_t \odot C_{t-1} + I_t \odot \tilde C_t \\ \\

H_t = O_t \odot \tanh (C_t)
$$




![image-20220429194259173](C:\Users\Five\Desktop\note\img\image-20220429194259173.png)












[^1]:《Deep learning in neural networks: An overview》 Schmidhuber, J . [J].Neural Networks
[^2]:《动手学深度学习》Release 2.0.0-beta0, Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola
[^3]:[[深度学习三巨头的综述：Deep Learning.pdf]]
[^4]:【循环神经网络】5分钟搞懂RNN，3D动画深入浅出https://www.bilibili.com/video/BV1z5411f7Bm
