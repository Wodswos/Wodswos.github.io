

![](C:/Users/Five/Desktop/note/img/letmeseesee.jpg)



# 典中典：波士顿房价预测

## 关于“回归”和“回归分析”

回归分析中的“回归”是什么意思？ - 颢卿的回答 - 知乎 https://www.zhihu.com/question/30123729/answer/554278766



跟很多舶来词一样，“回归”是“Regression”的直译，粗浅地来说，re表示back，gress等于go，所以“回归”可以某种程度上粗浅地理解为“go back to mean value”

> 想象这样一个场景，一堆看似无规律的数据在你的图像上肆无忌惮的乱走，我们要做的是找出其中的规律模型，把他们行走的趋势和轨迹“重新组合起来”。
>
> 回归分析中的“回归”是什么意思？ - 又又里的回答 - 知乎 https://www.zhihu.com/question/30123729/answer/46958971



Regression一词起源于生物学，后来被广泛用于**非确定性相互依赖关系--统计相关关系**。



> 《Linear Models and Generalizations: Least Squares and Alternatives》
>
> The literature meaning of REGRESSION is " to move in the backward direction"
>
> model exists in nature but is unknown to the experimenter.
>
> **“回归”的意思就是我们通过收集X与Y来确定实际上存在的关系模型**
>
> 回归分析中的“回归”是什么意思？ - Keven Howe的回答 - 知乎 https://www.zhihu.com/question/30123729/answer/47111877

## 线性回归和损失函数

假设影响房价的因素为居住面积、卧室个数、客厅个数、卫生间个数等，分别记为 $x_1,x_2,x_3,x_4$ .

假设不同因素之间彼此独立，成交价是不同因素的加权和，即有
$$
y = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 +b
$$
也可以进一步表示成向量形式：
$$
\mathbf x = [x_1,x_2,\dots ,x_n] ^ {\mathrm T},\mathbf w =[w_1,w_2,\dots,w_n] ^{\mathrm T} \\
y = \mathbf w^{\mathrm T} \mathbf x + b
$$

> 可以通过如下操作将偏差引入权重，使之成为权重的一部分，式子会更整齐
> $$
> \mathbf x' =[\mathbf x , 1] = [x_1,x_2,x_3,x_4,1] \\
> \mathbf w'= [\mathbf w , b] = [w_1,w_2,w_3,w_4,b]
> $$

再定义损失函数为平方损失：
$$
l(y,\hat y )= \frac 1 2 (y-\hat y) ^2
$$
（有个 $\frac 1 2$ 是为了后续求导方便消去）



## （训练）数据集的平均损失

输入显然不会只有一个样本，故假设有 $n$ 个样本，记
$$
\mathbf X = [\mathbf x_1, \mathbf x_2,\dots,\mathbf x_n] ^{\mathrm T}
\qquad 
\mathbf y = [\mathbf y_1,\mathbf y_2, \dots ,\mathbf y_n] ^{\mathrm T}
$$
则训练损失为
$$
l (\mathbf X,\mathbf y ,\mathbf w, b) = \frac 1 {2n} \sum _{i=1} ^n (y_i - \mathbf x_i ^{\mathrm T} \mathbf w - b)^2
$$
最小化损失 $l$ 来学习参数 $\mathbf w$ 和 $b$ .
$$
\mathbf w ^* ,b* = \arg \min _{\mathbf w,b} l (\mathbf X, \mathbf y,\mathbf w, b)
$$


## 梯度下降法

> 线性回归较为简单，故其可以有显式解。
>
> 但考虑到后续更复杂模型的求解，梯度下降是更为一般的求解方法。

理论就不在此赘述了，直接暴力地举个例子。

假设共有如下5个样本
$$
x_1 = 135,x_2 = 2, x_3 = 2, x_4 = 2 , y = 580000 \\
x_1 = 245,x_2 = 4, x_3 = 3, x_4 = 3 , y = 1200000 \\
x_1 = 90,x_2 = 2, x_3 = 2, x_4 = 1 , y = 200000 \\
x_1 = 110,x_2 = 3, x_3 = 2, x_4 = 2 , y = 250000 \\
x_1 = 132,x_2 = 3, x_3 = 2, x_4 = 2 , y = 320000
$$
$\mathbf w$ 初始化为零向量。

则有
$$
\frac {\partial l} {\partial y} = y - \hat y \\
\frac {\partial y} {\partial {\mathbf w} } = 
$$








$$
\mathbf w_t = w_{t-1} - \eta \frac {\partial l} {\partial w_{t-1}}
$$


## 批量化

用**采样后的样本的平均误差**作为**全样本的平均误差**的近似。



不能太小：每次计算量太小，不适合并行来最大化利用资源

不能太大：内存消耗增加，浪费计算（如样本都相同的极端情况）





> 小批量梯度下降是深度学习默认的求解算法。

## 代码实现





# 激活函数



## 带激活函数的自动求导









## 不同的激活函数

* Sigmoid
* Tanh
* ReLU



## 激活函数和初始化策略



# 损失函数

https://www.bilibili.com/video/BV1K64y1Q7wu?p=2

* 均方损失（L2 Loss）
  * 似然函数是高斯分布
* 绝对值损失（L1 loss）
* Huber's Robust Loss
  * 综合前两者，预测值和真实值较为靠近时用均方损失，反之用绝对值损失







# 多层感知机

1958年，

![](C:/Users/Five/Desktop/note/img/example_network.svg)

## 梯度和求导






## 量变引起质变
