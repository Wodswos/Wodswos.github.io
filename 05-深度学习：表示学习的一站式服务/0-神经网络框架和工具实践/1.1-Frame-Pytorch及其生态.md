

pytorch官方文档：https://pytorch.org/docs/stable/index.html

pytorch中文文档：https://pytorch-cn.readthedocs.io/zh/latest/

# 数据和模型参数读写

## torch.utils.data

### torch.utils.data.Dataset

表示Dataset的抽象类，子类至少应该Override其中`__len__()`和`__getitem__()`方法。







## model和参数的保存/checkpoint

### pt、pth、pkl

三者只是后缀不同，在格式上并无不同。

在用`torch.save()`保存函数时，每个人可以根据喜好不同命名为这三种后缀的任意一种。

一般管理使用`.pth`，但官方文档里更多的是 `.pt`，总而言之基本上无所谓。



### 保存模型和参数 or 只保存参数

保存模型和参数/加载模型和参数

```python
torch.save(model, mymodel.pt)

model = torch.load(mymodel.pt)
model.eval()
```

只保存参数/加载模型和参数

```python
torch.save(model.state_dict(), mymodel.pt)

model = My_model (*args, **kwargs)
model.load_state_dict(torch.load(mymodel.pth))
model.eval()
```



# 网络层的定义：torch.nn

## nn.Sequential

对于一般的序列模型，可以用`nn.sequential`类直接实现。

## nn.Module

pytorch中其实一般没有特别明显的Layer和Module的区别，不管是**自定义层、自定义块、自定义模型，都是通过继承Module类完成的**，这一点很重要。其实**Sequential类也是继承自Module类的。**

> 不同于pytorch，keras更常见的操作是通过继承Layer类来实现自定义层，不推荐去继承Model类定义模型，详细原因可以参见官方文档

> 也可以通过直接继承`torch.autograd.Function`类来自定义一个层，但不推荐。

### `__call__()`和`forward()`

　存在`__call__`方法的类对象被称为可调用对象，它的作用可以总结为以下两点：

* 简化了对象下方法的调用 (当某方法调用频率很高)
* 模糊了对象和函数调用时的区别 (提高了代码的兼容性)

当网络构建完之后，调`__call__`的时候，会去先调`forward`，即`__call__`其实是包了一层`forward`，所以会导致两者的功能类似



所以一般来说，自定义一个`Module`的时候，只要重写call和forward中的一个即可。

## 损失函数



# Torch Lightning

PyTorch Lightning is the deep learning framework for professional AI researchers and machine learning engineers who need maximal flexibility without sacrificing performance at scale.

```bash
pip install pytorch-lightning
```



# Tips

## 可视化

模型可视化工具有Visdom、TensorboardX、Pytorchviz、Netron

### Visdom





### TensorboardX

```bash
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorboard
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorboardX
```



#### colab使用TensroboardX

魔法函数

```python
%load_ext tensorboard
%tensorboard --logdir ./log 
```



### TorchViz





### PyTorch Profiler Tensorboard Plugin

https://github.com/pytorch/kineto/blob/main/tb_plugin/README.md

https://github.com/pytorch/kineto

## Context-Manager

比如

```python
torch.no_grad()
```

能主动disable gradient calculation，在确保不需要用到`Tensor.backward()`，只需要inference的时候可以使用，如validate，能够减小计算量。



## 其他特性



### 拷贝

![image-20220302093939042](C:\Users\Five\Desktop\note\img\image-20220302093939042.png)

`b = a.reshape()`这种操作是浅拷贝（类似于数据库创建了一个视图），所以对其的操作会修改原数据

可以通过显示地声明进行深拷贝

```python
B = A.clone()
```



