<!DOCTYPE html>
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta charset="utf-8"/>
  <meta content="pandoc" name="generator"/>
  <meta content="width=device-width, initial-scale=1.0, user-scalable=yes" name="viewport"/>
  <meta content="fivezehua@qq.com" name="author"/>
  <title>
   回归模型-从线性回归到逻辑斯蒂回归模型与最大熵模型
  </title>
  <style>
   html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
   #TOC {
    border-left: gray 2px dashed;
    padding: 5px;
    margin: 2%;

    right: 0;
    position: fixed;
    top: 5%;
    max-height: 90%;

    overflow: scroll;
  }
  #TOC::-webkit-scrollbar{
    width: 0!important;
  }
  #TOC a {
    color: #222222;
    font-size: 14px;
  }
  

    #main-navigation{
        position:fixed;
        left: 0;
        top: 0;
        width: 100%;

        z-index: 999;
    }

    #navigation-list-wrapper{
        display: flex;
        background-color: #ffffff;
        height: 35px;

        font-family: 'Times New Roman';
        font-size: 15px;
    }
    .main-topic-div{
        height: 40px;
        font-size: 20px;
    }

        .web-header{
          text-decoration: none;
          font-size: 25px;
        }

        .hutao-img{
            /* margin-right: 2%; */
            height: 30px;
            width: 30px;
            border-radius: 5px;
            margin-right: 5px;
        }

    .navigation-topic{
        /* display: inline; */
        background-color: #fdfdfd;
        border: #fdfdfd;
        font-size: 10px;
        overflow: hidden;
        margin-left: 2%;

        flex: 1;
    }
    .navigation-topic:hover{
        overflow: visible;
    }

    .drop-item, .main-topic-div{
        padding: 2px;
        background-color: #ffffff;
    }

    .drop-item a, .main-topic-div a{
        text-decoration: none;
        border-radius: 5px;
        font-size: 15px;
    }
    .drop-item a:hover, .main-topic-div a:hover{
        background-color: #e6e6e6;
    }


    #sub-navigation{
        border-right: #e6e6e6 solid 1px;
        position: fixed;
        left: 0;
        max-height: 90%;
        max-width: 20%;

        overflow: scroll;
    }
    #sub-navigation::-webkit-scrollbar{
        width: 0!important;
      }
    #sub-navigation li{
        list-style: none;
    }
    #sub-navigation a{
        color: #222222;
        text-decoration: none;
        font-size: 12px;
    }
    #sub-navigation a:hover{
        color: #606060;
        text-decoration: underline;
    }
    #current-path{
        background-color: #e6e6e6;
        border-top: #aaaaaa solid 1px;
        border-bottom: #aaaaaa solid 1px;
    }
  </style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default" type="text/javascript">
  </script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
 </head>
 <body>
  <tag id="sub-navigation">
   <li>
    <a href="https://wodswos.github.io/04-统计学习方法/index.html">
     04-统计学习方法
    </a>
    <ul>
     <li>
      <a href="https://wodswos.github.io/04-统计学习方法/0-人工智能先验知识和早期实践/index.html">
       0-人工智能先验知识和早期实践
      </a>
      <ul>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-人工智能先验知识和早期实践/index.html">
         index.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-人工智能先验知识和早期实践/基于符号的机器学习.html">
         基于符号的机器学习.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-人工智能先验知识和早期实践/控制论-现代控制论.html">
         控制论-现代控制论.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-人工智能先验知识和早期实践/控制论-经典控制论.html">
         控制论-经典控制论.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-人工智能先验知识和早期实践/数学篇-微积分.html">
         数学篇-微积分.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-人工智能先验知识和早期实践/数学篇-最优化理论.html">
         数学篇-最优化理论.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-人工智能先验知识和早期实践/数学篇-概率.html">
         数学篇-概率.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-人工智能先验知识和早期实践/数学篇-线性代数.html">
         数学篇-线性代数.html
        </a>
       </li>
      </ul>
     </li>
     <li>
      <a href="https://wodswos.github.io/04-统计学习方法/0-数据预处理/index.html">
       0-数据预处理
      </a>
      <ul>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-数据预处理/0.0-数据读写.html">
         0.0-数据读写.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-数据预处理/0.1-EDA-Exploratory Data Analysis.html">
         0.1-EDA-Exploratory Data Analysis.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-数据预处理/0.1-数据集的处理.html">
         0.1-数据集的处理.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-数据预处理/index.html">
         index.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-数据预处理/one-hot编码.html">
         one-hot编码.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-数据预处理/离散化.html">
         离散化.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-数据预处理/规约.html">
         规约.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-数据预处理/降维和PCA主成分分析.html">
         降维和PCA主成分分析.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/0-数据预处理/集成.html">
         集成.html
        </a>
       </li>
      </ul>
     </li>
     <li>
      <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/index.html">
       1-监督学习
      </a>
      <ul>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/0.0-模型的选择和优化.html">
         0.0-模型的选择和优化.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/0.1-最优化求解基本方法-梯度下降.html">
         0.1-最优化求解基本方法-梯度下降.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/1.1-分类模型-决策树.html">
         1.1-分类模型-决策树.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/1.2-分类模型-SVM.html">
         1.2-分类模型-SVM.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/1.2-分类模型-感知机.html">
         1.2-分类模型-感知机.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/1.3-分类模型-朴素贝叶斯.html">
         1.3-分类模型-朴素贝叶斯.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/1.4-分类模型-K近邻.html">
         1.4-分类模型-K近邻.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/2.1-回归模型-从线性回归到逻辑斯蒂回归模型与最大熵模型.html">
         2.1-回归模型-从线性回归到逻辑斯蒂回归模型与最大熵模型.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/3.1-集成学习-Bagging.html">
         3.1-集成学习-Bagging.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/3.2-集成学习-Boosting.html">
         3.2-集成学习-Boosting.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/3.3-集成学习-随机森林.html">
         3.3-集成学习-随机森林.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/4-EM算法.html">
         4-EM算法.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/5-隐马尔科夫模型.html">
         5-隐马尔科夫模型.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/6-条件随机场.html">
         6-条件随机场.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/index.html">
         index.html
        </a>
       </li>
      </ul>
     </li>
     <li>
      <a href="https://wodswos.github.io/04-统计学习方法/2-半监督学习和无监督学习/index.html">
       2-半监督学习和无监督学习
      </a>
      <ul>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/2-半监督学习和无监督学习/1-聚类-概述.html">
         1-聚类-概述.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/2-半监督学习和无监督学习/1-聚类-谱聚类.html">
         1-聚类-谱聚类.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/2-半监督学习和无监督学习/2-奇异值分解.html">
         2-奇异值分解.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/2-半监督学习和无监督学习/3-主成分分析.html">
         3-主成分分析.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/2-半监督学习和无监督学习/4-概率潜在语义分析.html">
         4-概率潜在语义分析.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/2-半监督学习和无监督学习/4-潜在语义分析.html">
         4-潜在语义分析.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/2-半监督学习和无监督学习/5-马尔科夫链蒙特卡洛法.html">
         5-马尔科夫链蒙特卡洛法.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/2-半监督学习和无监督学习/6-潜在迪利克雷分配.html">
         6-潜在迪利克雷分配.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/2-半监督学习和无监督学习/7-PageRank算法.html">
         7-PageRank算法.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/2-半监督学习和无监督学习/index.html">
         index.html
        </a>
       </li>
      </ul>
     </li>
     <li>
      <a href="https://wodswos.github.io/04-统计学习方法/3-强化学习/index.html">
       3-强化学习
      </a>
      <ul>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/3-强化学习/index.html">
         index.html
        </a>
       </li>
      </ul>
     </li>
     <li>
      <a href="https://wodswos.github.io/04-统计学习方法/4-实践案例/index.html">
       4-实践案例
      </a>
      <ul>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/4-实践案例/天池竞赛-二手车交易价格预测.html">
         天池竞赛-二手车交易价格预测.html
        </a>
       </li>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/4-实践案例/李宏毅机器学习-hw1.html">
         李宏毅机器学习-hw1.html
        </a>
       </li>
      </ul>
     </li>
     <li>
      <a href="https://wodswos.github.io/04-统计学习方法/index.html">
       index.html
      </a>
     </li>
     <li>
      <a href="https://wodswos.github.io/04-统计学习方法/x-Python的机器学习库.html">
       x-Python的机器学习库.html
      </a>
     </li>
     <li>
      <a href="https://wodswos.github.io/04-统计学习方法/其他/index.html">
       其他
      </a>
      <ul>
       <li>
        <a href="https://wodswos.github.io/04-统计学习方法/其他/关联分析/index.html">
         关联分析
        </a>
        <ul>
         <li>
          <a href="https://wodswos.github.io/04-统计学习方法/其他/关联分析/Apriori和FP增长算法.html">
           Apriori和FP增长算法.html
          </a>
         </li>
         <li>
          <a href="https://wodswos.github.io/04-统计学习方法/其他/关联分析/index.html">
           index.html
          </a>
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li>
      <a href="https://wodswos.github.io/04-统计学习方法/损失函数的设计.html">
       损失函数的设计.html
      </a>
     </li>
    </ul>
   </li>
  </tag>
  <div id="main-navigation">
   <div id="navigation-list-wrapper">
    <a class="web-header" href="https://wodswos.github.io">
     <img alt="hutao-img" class="hutao-img" src="https://zehua-markdown.oss-cn-shanghai.aliyuncs.com/logo.jpg"/>
     zehua's website
    </a>
    <div class="navigation-topic">
     <div class="main-topic-div">
      <a href="https://wodswos.github.io/01-数理基础/index.html">
       数理基础
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/01-数理基础/00-数学史概要.html">
       数学史概要
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/01-数理基础/01-数理且哲学66/index.html">
       01-数理且哲学66
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/01-数理基础/02-变化且分析77/index.html">
       02-变化且分析77
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/01-数理基础/03-结构且代数88/index.html">
       03-结构且代数88
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/01-数理基础/04-随机且统计99/index.html">
       04-随机且统计99
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/01-数理基础/05-其他/index.html">
       05-其他
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/01-数理基础/06-应用数学-信息论和编码/index.html">
       06-应用数学-信息论和编码
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/01-数理基础/06-物理/index.html">
       06-物理
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/01-数理基础/supplement-关于数学符号.html">
       关于数学符号
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/01-数理基础/supplement-关于追问本质和追问直观.html">
       关于追问本质和追问直观
      </a>
     </div>
    </div>
    <div class="navigation-topic">
     <div class="main-topic-div">
      <a href="https://wodswos.github.io/02-计算机科学基础/index.html">
       计算机科学基础
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/0-天才们的构思-计算机的理论大厦/index.html">
       0-天才们的构思-计算机的理论大厦
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/0x-《Crash Course Computer Science》笔记.html">
       《Crash Course Computer Science》笔记
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/0x-从半导体萌芽到互联网时代的所有巨头.html">
       从半导体萌芽到互联网时代的所有巨头
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/0x-模拟计算机会卷土重来吗？.html">
       模拟计算机会卷土重来吗？
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/1-硬件族谱-计算机组成原理/index.html">
       1-硬件族谱-计算机组成原理
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/1-面向硬件编程-汇编/index.html">
       1-面向硬件编程-汇编
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/2-编译器和编译原理/index.html">
       2-编译器和编译原理
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/3-硬件资源的统一管理-操作系统/index.html">
       3-硬件资源的统一管理-操作系统
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/4-面向编译器和解释器编程-C++特别篇/index.html">
       4-面向编译器和解释器编程-C++特别篇
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/4-面向编译器和解释器编程-其他高级计算机语言/index.html">
       4-面向编译器和解释器编程-其他高级计算机语言
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/5-计算科学的艺术-数据结构与算法/index.html">
       5-计算科学的艺术-数据结构与算法
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/6-信息革命-计算机网络/index.html">
       6-信息革命-计算机网络
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/7-设计和架构-软件工程瞎扯淡/index.html">
       7-设计和架构-软件工程瞎扯淡
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/8-拾遗和杂谈/index.html">
       8-拾遗和杂谈
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/02-计算机科学基础/计算机编年史.html">
       计算机编年史
      </a>
     </div>
    </div>
    <div class="navigation-topic">
     <div class="main-topic-div">
      <a href="https://wodswos.github.io/03-分布式系统和并行计算/index.html">
       分布式系统和并行计算
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/03-分布式系统和并行计算/00-并行计算/index.html">
       00-并行计算
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/03-分布式系统和并行计算/01-分布式和一致性/index.html">
       01-分布式和一致性
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/03-分布式系统和并行计算/02-分布式通信和协同/index.html">
       02-分布式通信和协同
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/03-分布式系统和并行计算/0x-主流实现和应用/index.html">
       0x-主流实现和应用
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/03-分布式系统和并行计算/0x-云计算.html">
       云计算
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/03-分布式系统和并行计算/0x-算一笔帐-量化计算机资源.html">
       算一笔帐-量化计算机资源
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/03-分布式系统和并行计算/0x-边缘计算.html">
       边缘计算
      </a>
     </div>
    </div>
    <div class="navigation-topic">
     <div class="main-topic-div">
      <a href="https://wodswos.github.io/04-统计学习方法/index.html">
       统计学习方法
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/04-统计学习方法/0-人工智能先验知识和早期实践/index.html">
       0-人工智能先验知识和早期实践
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/04-统计学习方法/0-数据预处理/index.html">
       0-数据预处理
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/04-统计学习方法/1-监督学习/index.html">
       1-监督学习
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/04-统计学习方法/2-半监督学习和无监督学习/index.html">
       2-半监督学习和无监督学习
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/04-统计学习方法/3-强化学习/index.html">
       3-强化学习
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/04-统计学习方法/4-实践案例/index.html">
       4-实践案例
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/04-统计学习方法/x-Python的机器学习库.html">
       Python的机器学习库
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/04-统计学习方法/其他/index.html">
       其他
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/04-统计学习方法/损失函数的设计.html">
       损失函数的设计
      </a>
     </div>
    </div>
    <div class="navigation-topic">
     <div class="main-topic-div">
      <a href="https://wodswos.github.io/05-深度学习方法/index.html">
       深度学习方法
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/05-深度学习方法/0-神经网络基础/index.html">
       0-神经网络基础
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/05-深度学习方法/0-神经网络框架和工具实践/index.html">
       0-神经网络框架和工具实践
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/05-深度学习方法/2.1-应用-数字图像处理：基础方法/index.html">
       2.1-应用-数字图像处理：基础方法
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/05-深度学习方法/2.2-应用-数字图像处理：计算机视觉/index.html">
       2.2-应用-数字图像处理：计算机视觉
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/05-深度学习方法/2.3-应用-自然语言处理/index.html">
       2.3-应用-自然语言处理
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/05-深度学习方法/x-脉冲神经网络.html">
       脉冲神经网络
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/05-深度学习方法/生活里的“智能”和“迷惑行为”.html">
       生活里的“智能”和“迷惑行为”
      </a>
     </div>
    </div>
    <div class="navigation-topic">
     <div class="main-topic-div">
      <a href="https://wodswos.github.io/06-工欲善其事必先利其器/index.html">
       工欲善其事必先利其器
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/06-工欲善其事必先利其器/0.0-Plan/index.html">
       0.0-Plan
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/06-工欲善其事必先利其器/Git-Action-Workflow和持续集成.html">
       Action-Workflow和持续集成
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/06-工欲善其事必先利其器/Git-项目协同的典范.html">
       项目协同的典范
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/06-工欲善其事必先利其器/Linux/index.html">
       Linux
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/06-工欲善其事必先利其器/Python-胶水和自动化/index.html">
       Python-胶水和自动化
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/06-工欲善其事必先利其器/Visio.html">
       Visio
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/06-工欲善其事必先利其器/代码复用和包管理工具.html">
       代码复用和包管理工具
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/06-工欲善其事必先利其器/收纳和整理.html">
       收纳和整理
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/06-工欲善其事必先利其器/笔记：从信息到知识，从编辑到管理/index.html">
       笔记：从信息到知识，从编辑到管理
      </a>
     </div>
    </div>
    <div class="navigation-topic">
     <div class="main-topic-div">
      <a href="https://wodswos.github.io/07-技多不压身/index.html">
       技多不压身
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/0.0-哲学和认知/index.html">
       0.0-哲学和认知
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/0.1-生命、人类、社会/index.html">
       0.1-生命、人类、社会
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/0.2-社会分配：政治、经济/index.html">
       0.2-社会分配：政治、经济
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/0.3-系统科学/index.html">
       0.3-系统科学
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/1.1-英语/index.html">
       1.1-英语
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/1.2-读书笔记/index.html">
       1.2-读书笔记
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/1.3-养生式健身/index.html">
       1.3-养生式健身
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/2.1-干饭人干饭魂/index.html">
       2.1-干饭人干饭魂
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/2.2-关于文字/index.html">
       2.2-关于文字
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/2.2-独立生活/index.html">
       2.2-独立生活
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/3.1-吃喝玩乐/index.html">
       3.1-吃喝玩乐
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/s-fiction/index.html">
       s-fiction
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/打工人的事情：劳务和社保.html">
       打工人的事情：劳务和社保
      </a>
     </div>
     <div class="drop-item">
      <a href="https://wodswos.github.io/07-技多不压身/护肤基础常识.html">
       护肤基础常识
      </a>
     </div>
    </div>
   </div>
   <div id="current-path">
   </div>
  </div>
  <header id="title-block-header">
   <h1 class="title">
    回归模型-从线性回归到逻辑斯蒂回归模型与最大熵模型
   </h1>
   <p class="author">
    fivezehua@qq.com
   </p>
  </header>
  <nav id="TOC" role="doc-toc">
   <ul>
    <li>
     <a href="#线性回归---linear-regression" id="toc-线性回归---linear-regression">
      线性回归 - Linear Regression
     </a>
     <ul>
      <li>
       <a href="#损失函数" id="toc-损失函数">
        损失函数
       </a>
      </li>
      <li>
       <a href="#梯度下降法适合计算机的最优化算法" id="toc-梯度下降法适合计算机的最优化算法">
        梯度下降法：适合计算机的最优化算法
       </a>
       <ul>
        <li>
         <a href="#单个样本时的梯度" id="toc-单个样本时的梯度">
          单个样本时的梯度
         </a>
        </li>
        <li>
         <a href="#多个样本时的梯度" id="toc-多个样本时的梯度">
          多个样本时的梯度
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a href="#计算优化批量化" id="toc-计算优化批量化">
        计算优化：批量化
       </a>
      </li>
      <li>
       <a href="#经典案例波士顿房价预测" id="toc-经典案例波士顿房价预测">
        经典案例：波士顿房价预测
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a href="#使用多项式特征" id="toc-使用多项式特征">
      使用多项式特征
     </a>
     <ul>
      <li>
       <a href="#新的问题" id="toc-新的问题">
        新的问题
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a href="#关于-logistic-回归模型的名称和起源" id="toc-关于-logistic-回归模型的名称和起源">
      关于 logistic
回归模型的名称和起源
     </a>
     <ul>
      <li>
       <a href="#高斯分布和-logistic-分布" id="toc-高斯分布和-logistic-分布">
        高斯分布和 logistic 分布
       </a>
      </li>
      <li>
       <a href="#和神经网络的关系" id="toc-和神经网络的关系">
        和神经网络的关系
       </a>
      </li>
      <li>
       <a href="#一些常见问题" id="toc-一些常见问题">
        一些常见问题
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </nav>
  <blockquote>
   <p>
    回归分析中的“回归”是什么意思？ - 颢卿的回答 - 知乎
https://www.zhihu.com/question/30123729/answer/554278766
   </p>
   <p>
    跟很多舶来词一样，“回归”是“Regression”的直译，粗浅地来说，re表示back，gress等于go，所以“回归”可以某种程度上粗浅地理解为“go
back to mean value”
   </p>
  </blockquote>
  <blockquote>
   <p>
    想象这样一个场景，一堆看似无规律的数据在你的图像上肆无忌惮的乱走，我们要做的是找出其中的规律模型，把他们行走的趋势和轨迹“重新组合起来”。
   </p>
   <p>
    回归分析中的“回归”是什么意思？ - 又又里的回答 - 知乎
https://www.zhihu.com/question/30123729/answer/46958971
   </p>
  </blockquote>
  <blockquote>
   <p>
    Regression一词起源于生物学，后来被广泛用于
    <strong>
     非确定性相互依赖关系–统计相关关系
    </strong>
    。
   </p>
  </blockquote>
  <blockquote>
   <p>
    《Linear Models and Generalizations: Least Squares and
Alternatives》
   </p>
   <p>
    The literature meaning of REGRESSION is ” to move in the backward
direction”
   </p>
   <p>
    model exists in nature but is unknown to the experimenter.
   </p>
   <p>
    <strong>
     “回归”的意思就是我们通过收集X与Y来确定实际上存在的关系模型
    </strong>
   </p>
   <p>
    回归分析中的“回归”是什么意思？ - Keven Howe的回答 - 知乎
https://www.zhihu.com/question/30123729/answer/47111877
   </p>
  </blockquote>
  <p>
   在机器学习中，回归一般就是指输出为连续值的模型。
  </p>
  <p>
   回归模型可以应用于如股票预测（价格是连续值），自动驾驶（输入感知器收集的数据，输出方向盘的角度和车速），推荐系统（输入用户信息、商品信息，输出用户的购买可能性）。
  </p>
  <h1 id="线性回归---linear-regression">
   线性回归 - Linear Regression
  </h1>
  <p>
   <span class="math display">
    \[
y = b + \sum w_i x_i
\]
   </span>
  </p>
  <h2 id="损失函数">
   损失函数
  </h2>
  <p>
   <span class="math display">
    \[
L(w,b) = \sum  _{i=1} ^n (\hat y _i -(b + \mathbf w\cdot \mathbf
{x_i}))^2\\
w^* ,b^* = \arg \min \sum _{i=1} ^n (\hat y _i - (b+ \mathbf w \cdot
\mathbf {x_i}))^2
\]
   </span>
  </p>
  <p>
   <span class="math display">
    \[
\mathbf x = [x_1,x_2,\dots ,x_n] ^ {\mathrm T},\mathbf w
=[w_1,w_2,\dots,w_n] ^{\mathrm T} \\
y = \mathbf w^{\mathrm T} \mathbf x + b
\]
   </span>
  </p>
  <blockquote>
   <p>
    可以通过如下操作将偏差引入权重，使之成为权重的一部分，式子会更整齐
    <span class="math display">
     \[
\mathbf x' =[\mathbf x , 1] = [x_1,x_2,x_3,x_4,1] \\
\mathbf w'= [\mathbf w , b] = [w_1,w_2,w_3,w_4,b]
\]
    </span>
   </p>
  </blockquote>
  <p>
   再定义损失函数为平方损失：
   <span class="math display">
    \[
L(y,\hat y )= \frac 1 2 (y-\hat y) ^2
\]
   </span>
   （有个
   <span class="math inline">
    \(\frac 1 2\)
   </span>
   的系数是为了后续求导方便消去）
  </p>
  <p>
   输入显然不会只有一个样本，故假设有
   <span class="math inline">
    \(n\)
   </span>
   个样本，记
   <span class="math display">
    \[
\mathbf X = [\mathbf x_1, \mathbf x_2,\dots,\mathbf x_n] ^{\mathrm T}
\qquad
\mathbf y = [\mathbf y_1,\mathbf y_2, \dots ,\mathbf y_n] ^{\mathrm T}
\]
   </span>
   则训练损失为
   <span class="math display">
    \[
L (\mathbf X,\mathbf y ,\mathbf w, b) = \frac 1 {2n} \sum _{i=1} ^n (y_i
- \mathbf x_i ^{\mathrm T} \mathbf w - b)^2
\]
   </span>
   最小化损失
   <span class="math inline">
    \(L\)
   </span>
   来学习参数
   <span class="math inline">
    \(\mathbf w\)
   </span>
   和
   <span class="math inline">
    \(b\)
   </span>
   .
   <span class="math display">
    \[
\mathbf w ^* ,b^* = \arg \min _{\mathbf w,b} L (\mathbf X, \mathbf
y,\mathbf w, b)
\]
   </span>
  </p>
  <h2 id="梯度下降法适合计算机的最优化算法">
   梯度下降法：适合计算机的最优化算法
  </h2>
  <blockquote>
   <p>
    线性回归较为简单，故其可以有显式解。
   </p>
   <p>
    但考虑到后续更复杂模型的求解，梯度下降是更为一般的求解方法。
   </p>
  </blockquote>
  <blockquote>
   <p>
    对线性回归使用梯度下降时，损失函数是 convex
的，故总能找打全局最优解，而不会陷入局部最优解。
   </p>
  </blockquote>
  <h3 id="单个样本时的梯度">
   单个样本时的梯度
  </h3>
  <p>
   在训练过程中，输入
   <span class="math inline">
    \(x\)
   </span>
   是参数（函数的系数），权重
   <span class="math inline">
    \(w\)
   </span>
   和偏置
   <span class="math inline">
    \(b\)
   </span>
   是变量，即有如下
   <strong>
    关于权重
    <span class="math inline">
     \(w_i\)
    </span>
    和
    <span class="math inline">
     \(b\)
    </span>
    的多元函数
   </strong>
   ：
   <span class="math display">
    \[
L(w_1, \dots, w_n, b) = \frac 1 2 [(x_1w_1 + x_2 w_2 + \dots + x_i w_i +
\dots + x_nw_n + 1\cdot b) -\hat y] ^2
\]
   </span>
   故损失函数
   <span class="math inline">
    \(L\)
   </span>
   对某一个权重值
   <span class="math inline">
    \(w_i\)
   </span>
   （偏置
   <span class="math inline">
    \(b\)
   </span>
   可以看作一个系数恒为
   <span class="math inline">
    \(1\)
   </span>
   的特殊权重值）的偏导为：
   <span class="math display">
    \[
\frac {\partial L} {\partial w_i} = [\frac 1 2 (x_i w_i + t_i - \hat y
)^2]' = x_i ^2 w_i + x_i(t_i- \hat y) \\
\]
   </span>
   其中，
   <span class="math inline">
    \(t_i = x_1 w_1 + x_2w_2 +
\dots + x_{i-1}w_{i-1} + x_{i+1} w_{i+1} + \dots +x_nw_n +
b\)
   </span>
  </p>
  <p>
   故由大一水平的高数知识可知，表示梯度的向量为
   <span class="math display">
    \[
(\frac {\partial L} {\partial w_1} ,\frac {\partial L} {\partial w_2},
\dots ,\frac {\partial L} {\partial w_n}, \frac {\partial L} {\partial
b} )
\]
   </span>
   前式带入后式即可。上述讨论输入仅为单个数据，实际中我们要考虑的是整个数据集的损失函数，故进一步讨论。
  </p>
  <h3 id="多个样本时的梯度">
   多个样本时的梯度
  </h3>
  <p>
   <span class="math display">
    \[
L(w,b) = \frac 1 2\sum  _{i=1} ^n (\hat y _i -(b + \mathbf w\cdot
\mathbf {x_i}))^2
\]
   </span>
  </p>
  <p>
   故有
   <span class="math display">
    \[
\frac {\partial L}{\partial w_i} = \frac  {\partial \sum _{j=1} ^n [
\frac 1 2(\hat y_j - t_j) ^2  + (t_j -\hat y_j) x_{ij} w_i + \frac 1 2
x_{ij}^2 w_i ^2]} {\partial w_i}
= \sum _{j=1} ^n [x_{ij} ^2 w_i + x_{ij}(t_j -\hat y_j)]
\]
   </span>
  </p>
  <h2 id="计算优化批量化">
   计算优化：批量化
  </h2>
  <p>
   用
   <strong>
    采样后的样本的平均误差
   </strong>
   作为
   <strong>
    全样本的平均误差
   </strong>
   的近似。
  </p>
  <p>
   不能太小：每次计算量太小，不适合并行来最大化利用资源
  </p>
  <p>
   不能太大：内存消耗增加，浪费计算（如样本都相同的极端情况）
  </p>
  <blockquote>
   <p>
    小批量梯度下降是深度学习默认的求解算法。
   </p>
  </blockquote>
  <h2 id="经典案例波士顿房价预测">
   经典案例：波士顿房价预测
  </h2>
  <p>
   假设影响房价的因素为居住面积、卧室个数、客厅个数、卫生间个数等，分别记为
   <span class="math inline">
    \(x_1,x_2,x_3,x_4\)
   </span>
   .
  </p>
  <p>
   假设不同因素之间彼此独立，成交价是不同因素的加权和，即有
   <span class="math display">
    \[
y = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 +b
\]
   </span>
   也可以进一步表示成向量形式：
   <span class="math display">
    \[
\mathbf x = [x_1,x_2,\dots ,x_n] ^ {\mathrm T},\mathbf w
=[w_1,w_2,\dots,w_n] ^{\mathrm T} \\
y = \mathbf w^{\mathrm T} \mathbf x + b
\]
   </span>
  </p>
  <p>
   假设共有如下5个样本
   <span class="math display">
    \[
x_1 = 135,x_2 = 2, x_3 = 2, x_4 = 2 , y = 580000 \\
x_1 = 245,x_2 = 4, x_3 = 3, x_4 = 3 , y = 1200000 \\
x_1 = 90,x_2 = 2, x_3 = 2, x_4 = 1 , y = 200000 \\
x_1 = 110,x_2 = 3, x_3 = 2, x_4 = 2 , y = 250000 \\
x_1 = 132,x_2 = 3, x_3 = 2, x_4 = 2 , y = 320000
\]
   </span>
   <span class="math inline">
    \(\mathbf w\)
   </span>
   初始化为零向量。
  </p>
  <p>
   则有
   <span class="math display">
    \[
\frac {\partial L} {\partial y} = y - \hat y \\
\frac {\partial y} {\partial {\mathbf w} } =
\]
   </span>
  </p>
  <p>
   <span class="math display">
    \[
\mathbf w_t = w_{t-1} - \eta \frac {\partial l} {\partial w_{t-1}}
\]
   </span>
  </p>
  <h1 id="使用多项式特征">
   使用多项式特征
  </h1>
  <p>
   线性回归中，一般来说输出都是关于输入的一次函数
   <span class="math display">
    \[
y = b + w_1 x_1 + w_2 x_2 +\dots
\]
   </span>
   这意味着，「输入球的半径，输出球的体积」这样简单的算术关系，该模型都无法胜任。
  </p>
  <p>
   所以我们可能需要构建关于单个输入
   <span class="math inline">
    \(x\)
   </span>
   的多项式：
   <span class="math display">
    \[
y = b + (w_{11} x_1 + w_{12} x_1^2 + w_{13} x_1 ^3 + \dots)
+ (w_{21}x_2 + w_{22}x_2^2  + w_{23} x_2 ^3 +\dots) + \dots
\]
   </span>
  </p>
  <h2 id="新的问题">
   新的问题
  </h2>
  <p>
   如果多个输入量之间存在累乘关系，该如何处理。
  </p>
  <h1 id="关于-logistic-回归模型的名称和起源">
   关于 logistic
回归模型的名称和起源
  </h1>
  <p>
   高中的生物课本就出现过“种群增长的逻辑斯蒂模型”：
  </p>
  <p>
   其背后对应着这样一个微分方程：
   <span class="math display">
    \[
\frac {\mathrm d y} {\mathrm d t} =r_0 y(1-\frac y {y_m})
\]
   </span>
  </p>
  <h2 id="高斯分布和-logistic-分布">
   高斯分布和 logistic 分布
  </h2>
  <p>
   高斯分布的密度函数
   <span class="math display">
    \[
f(x) = \frac {1} {\sqrt {2\pi} \sigma} e ^{- \frac {(x-u)^2} {2\sigma
^2}}
\]
   </span>
  </p>
  <p>
   logistic 分布的密度函数要更复杂一些
   <span class="math display">
    \[
f(x) = \frac {e ^{-(x-u)/\gamma}} {\gamma (1 + e ^{-(x-u)/\gamma})^2}
\]
   </span>
  </p>
  <h2 id="和神经网络的关系">
   和神经网络的关系
  </h2>
  <p>
   大部分回归都是没有隐藏层的神经网络。
  </p>
  <h2 id="一些常见问题">
   一些常见问题
  </h2>
  <p>
   【机器学习】关于逻辑斯蒂回归，看这一篇就够了！解答绝大部分关于逻辑斯蒂回归的常见问题，以及代码实现
- 舟晓南的文章 - 知乎 https://zhuanlan.zhihu.com/p/463149304
  </p>
 </body>
</html>
